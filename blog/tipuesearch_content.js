var tipuesearch = {"pages":[{"title":"Jupyterhub SSH Tunnel","text":"Well, it turns out I could not get the Jupyterhub NGINX Reverse Proxy to work. I quote the following reverse-proxy settings in my NGINX configuration file, location /jupyterhub { proxy_pass http://127.0.0.1:8080; # port used right now proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # websocket headers proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } I could open a Jupyter notebook , but the kernel would not start! Luckily, I was able to perform an SSH tunnel. Here, I tunneled port 32397 from my local machine, to port 8080 on the server where the Jupyterhub server was running. I include the relevant lines in the jupyterhub_config.py configuration file, which sets up the port on which the Jupyterhub server listens, # This is the address on which the proxy will bind. Sets protocol, ip, base_url c . JupyterHub . bind_url = 'http://127.0.0.1:8080/jupyterhub' Here is the tunneling command, ssh -fN -L 32397:localhost:8080 tanim@192.168.1.100 Port 32397 is open on the local (SSH client) machine, and connects to port 8080 on the remote (SSH server) machine. I login with username tanim on SSH server 192.168.1.100 . The -fN flags means to start an SSH tunneled connection with logging into the shell. I then checked to see whether I could login, from my browser, to http://localhost:32397/jupyterhub . It turns out I can login fine, but my Jupyterhub server complains about a non-SSL connection. This error is harmless, because I have already firewalled away that port. Port 8080 on the SSH server is not visible to the outside world, except through an NGINX reverse proxy or through this SSH tunnel. The browser and server redirect to http://localhost:32397/jupyterhub/hub/login . Once I login, I can go to the Jupyter notebook located at ~/Desktop/covid19_movies_conus_states.ipynb . covid19_movies_conus_states.ipynb is a copy of the Google Colab hosted Jupyter notebook I created, that visualizes summary movies of the latest COVID-19 cases and deaths in the CONUS (Continental United States) and nine states. It is a copy of the public notebook covid19_movies_conus_states.ipynb that is hosted on Google Colab . Finally, I can load covid19_movies_conus_states.ipynb and start a Python 3 kernel!","tags":"computronics","url":"jupyterhub-ssh-tunnel.html","loc":"jupyterhub-ssh-tunnel.html"},{"title":"Plex Server Running Update","text":"Here is a running update on what exists in my Plex server. How to get started describes how to use the Plex server to get what you want. How to Get Started To join my Plex server, follow instructions from this blog article . To request TV shows and movies, go to my OMBI website , and login with username requester and password requester . Here's a screen shot, This will send you to the web page where you submit your movie and TV show requests. It even works with obscure TV shows, like Kaamelott . It's been used quite a bit , and quite well, in the time (since December 22, 2019 ) it's been operational! Summary of Media on Plex Server The summary of media on my Plex server is is organized into three sections: the TV part , the Movie part , and the Music part . I plan to run this frequently, until I iron enough of the bugs out, and it looks like what I want it to look like. Then I plan to run it weekly. TV Libraries As of January 19, 2021 , there are 25,591 TV episodes in 304 TV shows. The total size of TV media is 6.829 TB. The total duration of TV media is 1 year, 5 months, 31 days, 3 hours, 44 minutes, and 2.434 seconds. Since December 20, 2020 , I have added 412 TV epsisodes in 304 TV shows. The total size of TV media that I have added is 150.506 GB. The total duration of TV media that I have added is 12 days, 7 hours, 38 minutes, and 26.853 seconds. Here's a fan array of all the TV shows on my Plex server, organized by calendar year. SUMMARY OF 304 SHOWS; 25,535 EPISODES; BY 19 January 2021 Movie Libraries As of January 19, 2021 , there are 2,473 movies in 7 categories. The total size of movie media is 2.147 TB. The total duration of movie media is 6 months, 2 days, 56 minutes, and 44.172 seconds. Since December 20, 2020 , I have added 42 movies in 6 categories. The total size of movie media that I have added is 37.625 GB. The total duration of movie media that I have added is 2 days, 19 hours, 44 minutes, and 33.039 seconds. Here are the 7 movies I have most recently added. Three Colors: White (1994) , added on January 18, 2021. Three Colors: Red (1994) , added on January 18, 2021. Three Colors: Blue (1993) , added on January 18, 2021. The Brand New Testament (2015) , added on January 17, 2021. Once Upon a Snowman (2020) , added on January 17, 2021. The History of Time Travel (2014) , added on January 17, 2021. Tromeo & Juliet (1996) , added on January 16, 2021. Here is a summary by category. action : As of January 19, 2021 , there are 336 movies in this category. The total size of movie media here is 309.132 GB. The total duration of movie media here is 27 days, 6 hours, 14 minutes, and 37.584 seconds. Since December 20, 2020 , I have added 1 movies in this category. The total size of movie media I added here is 599.118 MB. The total duration of movie media I added here is 1 hour, 31 minutes, and 3.530 seconds. animation : As of January 19, 2021 , there are 202 movies in this category. The total size of movie media here is 164.018 GB. The total duration of movie media here is 11 days, 20 hours, 11 minutes, and 58.366 seconds. Since December 20, 2020 , I have added 7 movies in this category. The total size of movie media I added here is 5.639 GB. The total duration of movie media I added here is 9 hours, 3 minutes, and 10.768 seconds. comedy : As of January 19, 2021 , there are 1,046 movies in this category. The total size of movie media here is 903.365 GB. The total duration of movie media here is 2 months, 15 days, 19 hours, 26 minutes, and 30.404 seconds. Since December 20, 2020 , I have added 13 movies in this category. The total size of movie media I added here is 12.245 GB. The total duration of movie media I added here is 21 hours, 57 minutes, and 38.694 seconds. documentary : As of January 19, 2021 , there are 64 movies in this category. The total size of movie media here is 54.114 GB. The total duration of movie media here is 4 days, 3 hours, 17 minutes, and 26.502 seconds. Since December 20, 2020 , I have added 3 movies in this category. The total size of movie media I added here is 2.410 GB. The total duration of movie media I added here is 4 hours, 29 minutes, and 32.178 seconds. drama : As of January 19, 2021 , there are 613 movies in this category. The total size of movie media here is 578.740 GB. The total duration of movie media here is 1 month, 20 days, 39 minutes, and 47.611 seconds. Since December 20, 2020 , I have added 17 movies in this category. The total size of movie media I added here is 15.758 GB. The total duration of movie media I added here is 1 day, 4 hours, 12 minutes, and 1.736 seconds. horror : As of January 19, 2021 , there are 145 movies in this category. The total size of movie media here is 125.766 GB. The total duration of movie media here is 9 days, 15 hours, 28 minutes, and 14.265 seconds. science fiction : As of January 19, 2021 , there are 67 movies in this category. The total size of movie media here is 63.629 GB. The total duration of movie media here is 5 days, 7 hours, 38 minutes, and 9.440 seconds. Since December 20, 2020 , I have added 1 movies in this category. The total size of movie media I added here is 1012.004 MB. The total duration of movie media I added here is 2 hours, 31 minutes, and 6.133 seconds. Music Libraries As of January 19, 2021 , there are 18,151 songs made by 884 artists in 1,816 albums. The total size of music media is 309.410 GB. The total duration of music media is 7 months, 20 days, 5 hours, 18 minutes, and 4.786 seconds. Since December 20, 2020 , I have added 120 songs made by 21 artists in 25 albums. The total size of music media that I have added is 2.149 GB. The total duration of music media that I have added is 1 day, 15 hours, 8 minutes, and 31.341 seconds. Here's a collection of all the NPR Fresh Air episodes, for all the years, that I have been collecting. SUMMARY OF 18 YEARS OF NPR FRESH AIR (2004 - 2021) ALBUMS, UP TO 19 January 2021","tags":"persistent","url":"plex-server-running-update.html","loc":"plex-server-running-update.html"},{"title":"COVID-19 running update","text":"This article summarizes the status of the cuurrent COVID-19 status. The format comes from the demonstration README.rst for my COVID-19 statistics GitHub repository . Here are the latest summary COVID-19 statistics for the top 50 metropolitan statistical areas in the United States, COVID-19 STATS FOR 50 METROS TO 27 December 2020. RANK IDENTIFIER NAME POPULATION FIRST INC. NUM DAYS NUM CASES NUM DEATHS MAX CASE COUNTY MAX CASE COUNTY NAME 1 nyc NYC Metro Area 19,216,182 01 March 2020 301 1,040,457 47,084 413,472 New York City, New York 2 losangeles LA Metro Area 18,711,436 25 January 2020 337 1,264,161 14,827 720,099 Los Angeles County, California 3 chicago Chicago Metro Area 9,458,539 24 January 2020 338 688,658 12,646 384,931 Cook County, Illinois 4 dallas Dallas Metro Area 7,573,136 09 March 2020 293 457,413 4,697 186,880 Dallas County, Texas 5 houston Houston Metro Area 7,066,141 04 March 2020 298 340,668 4,554 231,707 Harris County, Texas 6 bayarea Bay Area 6,860,207 31 January 2020 331 216,258 2,182 64,974 Santa Clara County, California 7 dc DC Metro Area 6,280,487 05 March 2020 297 265,044 4,874 53,161 Prince George's County, Maryland 8 miami Miami Metro Area 6,166,488 06 March 2020 296 501,439 7,827 288,305 Miami-Dade County, Florida 9 philadelphia Philadelphia Metro Area 6,102,434 06 March 2020 296 300,090 8,183 89,887 Philadelphia County, Pennsylvania 10 atlanta Atlanta Metro Area 6,020,364 02 March 2020 300 317,763 4,325 54,635 Fulton County, Georgia 11 phoenix Phoenix Metro Area 4,948,203 26 January 2020 336 329,101 5,253 302,859 Maricopa County, Arizona 12 boston Boston Metro Area 4,873,019 01 February 2020 330 240,060 8,090 71,348 Middlesex County, Massachusetts 13 detroit Detroit Metro Area 4,319,629 10 March 2020 292 210,792 7,090 83,409 Wayne County, Michigan 14 seattle Seattle Metro Area 3,979,845 21 January 2020 341 109,163 1,665 60,695 King County, Washington 15 minneapolis Minneapolis Metro Area 3,640,043 06 March 2020 296 254,197 3,210 85,143 Hennepin County, Minnesota 16 sandiego San Diego Metro Area 3,338,330 10 February 2020 321 145,842 1,402 145,842 San Diego County, California 17 tampa Tampa Metro Area 3,194,831 01 March 2020 301 145,642 2,704 74,368 Hillsborough County, Florida 18 denver Denver Metro Area 2,967,239 05 March 2020 297 171,935 2,591 46,570 Denver County, Colorado 19 stlouis St. Louis Metro Area 2,803,228 07 March 2020 295 194,851 3,200 67,120 St. Louis County, Missouri 20 baltimore Baltimore Metro Area 2,800,053 08 March 2020 294 117,391 2,468 37,578 Baltimore County, Maryland 21 charlotte Charlotte Metro Area 2,636,883 11 March 2020 291 143,531 1,613 60,337 Mecklenburg County, North Carolina 22 orlando Orlando Metro Area 2,608,147 12 March 2020 290 129,403 1,612 73,049 Orange County, Florida 23 sanantonio San Antonio Metro Area 2,550,960 12 February 2020 319 134,506 2,128 112,230 Bexar County, Texas 24 portland Portland Metro Area 2,492,412 28 February 2020 303 66,012 827 24,581 Multnomah County, Oregon 25 sacramento Sacramento Metro Area 2,363,730 21 February 2020 310 88,322 1,052 62,434 Sacramento County, California 26 pittsburgh Pittsburgh Metro Area 2,317,600 13 March 2020 289 104,523 1,984 51,260 Allegheny County, Pennsylvania 27 lasvegas Las Vegas Metro Area 2,266,715 05 March 2020 297 164,673 2,261 164,673 Clark County, Nevada 28 austin Austin Metro Area 2,227,083 13 March 2020 289 85,166 937 48,424 Travis County, Texas 29 cincinnati Cincinnati Metro Area 2,221,208 14 March 2020 288 131,442 1,055 49,823 Hamilton County, Ohio 30 kansascity Kansas City Metro Area 2,157,990 07 March 2020 295 101,216 1,167 36,727 Johnson County, Kansas 31 columbus Columbus Metro Area 2,122,271 14 March 2020 288 128,202 1,077 80,939 Franklin County, Ohio 32 indianapolis Indianapolis Metro Area 2,074,537 06 March 2020 296 142,761 2,409 68,327 Marion County, Indiana 33 cleveland Cleveland Metro Area 2,048,449 09 March 2020 293 106,301 1,284 67,417 Cuyahoga County, Ohio 34 nashville Nashville Metro Area 1,934,317 05 March 2020 297 168,811 1,556 66,724 Davidson County, Tennessee 35 virginiabeach Virginia Beach Metro Area 1,768,901 09 March 2020 293 60,235 792 15,667 Virginia Beach city, Virginia 36 providence Providence Metro Area 1,624,578 14 March 2020 288 106,038 2,667 55,922 Providence County, Rhode Island 37 milwaukee Milwaukee Metro Area 1,575,179 11 March 2020 291 145,573 1,473 89,838 Milwaukee County, Wisconsin 38 jacksonville Jacksonville Metro Area 1,559,514 10 March 2020 292 88,382 1,114 58,296 Duval County, Florida 39 oklahomacity Oklahoma City Metro Area 1,408,950 13 March 2020 289 96,298 721 56,086 Oklahoma County, Oklahoma 40 raleigh Raleigh Metro Area 1,390,785 03 March 2020 299 55,491 476 41,523 Wake County, North Carolina 41 memphis Memphis Metro Area 1,346,045 08 March 2020 294 98,255 1,288 65,713 Shelby County, Tennessee 42 richmond Richmond Metro Area 1,291,900 12 March 2020 290 45,733 757 12,399 Chesterfield County, Virginia 43 neworleans New Orleans Metro Area 1,270,530 09 March 2020 293 75,394 1,958 30,031 Jefferson Parish, Louisiana 44 louisville Louisville/Jefferson County Metro Area 1,265,108 08 March 2020 294 79,779 1,007 50,679 Jefferson County, Kentucky 45 saltlakecity Salt Lake City Metro Area 1,232,696 25 February 2020 306 107,933 547 103,615 Salt Lake County, Utah 46 hartford Hartford Metro Area 1,204,877 14 March 2020 288 53,624 2,128 43,253 Hartford County, Connecticut 47 buffalo Buffalo Metro Area 1,127,983 15 March 2020 287 47,284 1,316 39,486 Erie County, New York 48 birmingham Birmingham Metro Area 1,090,435 13 March 2020 289 79,930 986 49,645 Jefferson County, Alabama 49 grandrapids Grand Rapids Metro Area 1,077,370 12 March 2020 290 68,501 894 42,379 Kent County, Michigan 50 rochester Rochester Metro Area 1,069,644 11 March 2020 291 40,086 704 31,460 Monroe County, New York Here is a table of the NYC and SF Bay Area MSA COVID-19 trends. COVID-19 latest trends video for MSAs New York City SF Bay Area","tags":"persistent","url":"covid19-running-update.html","loc":"covid19-running-update.html"},{"title":"Read the docs with Sphinx","text":"I have two projects, NPRStuff and Howdy , with fairly sophisticated Sphinx documentation and comprehensive GitHub repositories. There is a trickiness I had to cobble together, in order to do the following, The preamble to the Sphinx documentation should live in the repository's README.rst . The Sphinx documentation should build normally on one's own directory by running make html in the Sphinx docs directory. The Sphinx documentation should also build properly on Read the Docs . Here is what I had to do to make it happen. I incorporate relevant code snippets from the Howdy Sphinx documentation and included files. The README.rst looks as it is supposed to, and it generates just fine on GitHub . To get Read the Docs to properly build the Sphinx documentation, these are the two lines I needed to put into my Sphinx conf.py . is_in_readthedocs = ( os . environ . get ( 'READTHEDOCS' ) is not None ) tls_verify = is_in_readthedocs To get the preamble in the Sphinx docs to be the same, module formatting and style, as README.rst on Github, I had to put the following into the top of the Sphinx main index.rst . .. include :: ../../README.rst My directory structure looks like this: README.rst is in the top level, while index.rst lives in the docs/source . subdirectory. Hence, the ../../README.rst . And then things seemed to work smoothly.","tags":"computronics","url":"readthedocs-with-sphinx.html","loc":"readthedocs-with-sphinx.html"},{"title":"Getting Pelican YouTube Plugin to Work","text":"I wanted to embed a Quaker parrot describing 2020 , using a YouTube video I had uploaded. The pelican_youtube plugin seemed to be the way to go. There are (of course there are, in the absence of testing or real engineering) papercuts to get it working. Description follows. There is a pelican_youtube folder that contains the pelican_youtube plugin. My pelicanconf.py gives the location of the pelican plugins on my machine (the server that generates the static webpages): /mnt/software/sources/pelican-plugins . Follow instructions on how to add this plugin from the INSTALLATION section . Here is the relevant snippet from my pelicanconf.py , # Pelican plugins PLUGIN_PATHS = [ '/mnt/software/sources/pelican-plugins' ] PLUGINS = [ 'gravatar' , 'render_math' , 'pelican_youtube' ] Inside the /mnt/software/sources/pelican-plugins/pelican_youtube directory, make these symbolic links, ln -sf pelican_youtube/__init__.py . ln -sf pelican_youtube/youtube.py . Now when I ran pelican in my blog directory, the blog post generated properly. See for yourself ! Warning More Pelican papercuts! I had to make the following change in my pelicanconf.py to refer to articles by their slugs . # Defaults ARTICLE_URL = \" {slug} .html\" PAGE_URL = \" {slug} .html\" PAGE_SAVE_AS = \" {slug} .html\" I followed advice on referring to articles by their slugs from this StackOverflow article . Change I made was in the URL definition, .. _year_in_review: 2020-year-in-review.html Isn't coding fun? /s","tags":"computronics","url":"getting-pelican-youtube-to-work.html","loc":"getting-pelican-youtube-to-work.html"},{"title":"2020 Year in Review","text":"COVID-19...yeah. I first saw this post on Reddit here . It is funny and wholly appropriate. Since it's not on YouTube, here's the video! Party Quaker parrot telling us how he feels about 2020. Enjoy!","tags":"random","url":"2020-year-in-review.html","loc":"2020-year-in-review.html"},{"title":"Google Drive API: What the Everloving Fuck?","text":"Show of hands on if you think this is the logical work flow for getting child files of a Google drive directory through the Google Drive API : get the file ID of the directory. get the children of the directory, identified by its file ID . The Google Drive API has instructions on traversing through a directory's children at this website . These instructions are misleading (documentation is for Version 2 of the Google Drive API ). There is no children() method associated with the Google API Client Resource . Instead, following instructions from this StackOverflow post and the MIME types needed for the list() method query used in the necessary call (taken from this Google Drive API page ): Files with ID within a collection, e.g. parents collection '1234567' in parents Then the Python query to get the children of the Google drive whose ID is given by id_for_folder is, from googleapiclient.discovery import build ... drive = build ( 'drive' , 'v3' , http = http_auth , cache_discovery = False ) ... results_in_folder = drive_service . files ( ) . list ( pageSize = 1000 , q = \"' %s ' in parents\" % id_for_folder ) . execute ( ) At least, that's what worked for me. I cannot use the example code, because the data is restricted.","tags":"computronics","url":"google-drive-whatthefuck.html","loc":"google-drive-whatthefuck.html"},{"title":"Testing a new Google OAuth2 work flow","text":"This represents my efforts in trying out a new Google Blessed Python OAuth2 Flow , to supersede the older flow that used oauth2client (because of deprecation ). Here, I am going to use client_secrets.json from the Howdy SDK that I have developed, with my standard scopes. I am going to create a Googleapiclient Resource object that gets my own contacts, and then retrieve contacts using this object. Here are the steps. Create the Flow object this way, from google_auth_oauthlib.flow import Flow flow = Flow . from_client_secrets_file ( os . path . join ( resourceDir , 'client_secrets.json' ), scopes = [ 'https://www.googleapis.com/auth/gmail.send' , 'https://www.googleapis.com/auth/contacts.readonly' , 'https://www.googleapis.com/auth/youtube.readonly' , 'https://www.googleapis.com/auth/spreadsheets' , 'https://www.googleapis.com/auth/musicmanager' , 'https://www.googleapis.com/auth/drive' ], redirect_uri = \"urn:ietf:wg:oauth:2.0:oob\" ) Get the authorization URL from this Flow object, uri , _ = flow . authorization_url ( ) Go to that URL, grant all permissions, and copy over that access code, called authorization_code . Then grant authorization with this code, and retrieve the Credentials object called credentials . credentials = flow . fetch_token ( code = authorization_code ) Optionally , refresh the Credentials object with a new Request object that disables SSL verification . import requests from google.auth.transport.requests import Request s = requests . Session ( ) s . verify = False credentials . refresh ( Request ( s ) ) Maybe I am overthinking it?? OK, so now that I have a valid Credentials object, I can try to create a valid Resource object from it? from googleapiclient.discovery import build people_service = build ( 'people' , 'v1' , credentials = credentials , cache_discovery = False ) And perhaps I am fortunate not to error out? And finally, get the resulting contact info into a Python dictionary. connections = people_service . people ( ) . connections ( ) . list ( resourceName = 'people/me' , personFields = 'names,emailAddresses' , pageSize = pagesize ) . execute ( ) And if everything is copacetic, I won't get error messages?","tags":"computronics","url":"testing-new-google-oauth2-workflow.html","loc":"testing-new-google-oauth2-workflow.html"},{"title":"Git-LFS usage on my git repo","text":"Git-LFS is the correct way to store binary files (such as raster images, binary data, etc.). I am following instructions both from this website and parts of this older blog post . Add the following types of files to add to git-lfs git lfs track *.png *.gif After adding a bunch of binary formats that Git-LFS now handles, do a git add .gitattributes . This will track the binary formats from the current state of your git repo, but it won't do anything about the binary files from previous commits. Here is how to do so. Run this command, git lfs migrate import --everything --include=\"*.png\" --include=\"*.gif\" and other binary formats you wanted Git-LFS to track. Now fix the old info in your checked out repo, by running these commands: git reflog expire --expire-unreachable=now --all git prune gc Now follow advice given by this older blog post . Nuke what's in the previous remote git repo, by running, git push --set-upstream origin master --force On other local copies on another machines, obliterate the history by running, git fetch --all && git reset --hard origin/master git pull And then that's it!","tags":"computronics","url":"git-lfs-usage-on-my-git-repo.html","loc":"git-lfs-usage-on-my-git-repo.html"},{"title":"TRAMP the fuck??","text":"The tramp emacs package is a very good idea buried by shit-awful implementation. It hangs for unknown reasons in ways that are impossible to debug. The only thing that seemed to \"work\" when Emacs + TRAMP seizes up like a Parkinsonian fugue is to do C-g (Control-G in Emacs parlance) to unjam Emacs . Then I try to tramp-connect to other SSH servers, such as my local machine, and hope and prayers my way to a solution. Thanks to this email on the help-gnu-emacs mailing list for the insight.","tags":"computronics","url":"tramp-the-fuck.html","loc":"tramp-the-fuck.html"},{"title":"Such an eloquent turn of phrase","text":"I saw this phrase in this New York Times article, Facebook Can't Be Reformed . And I quote, The phrase is both a promise and a deflection. It's a plea for unearned trust -- give us time, we are working toward progress . And it cuts off meaningful criticism -- yes, we know this isn't enough, but more is coming . It applies to so many nothing-burger responses I get. I would continue a conversation, and a resolution, if the response goes as, \"I don't know\" or \"I can't or won't do it.\"","tags":"random","url":"such-an-eloquent-turn-of-phrase.html","loc":"such-an-eloquent-turn-of-phrase.html"},{"title":"Python 2 environments for Anaconda","text":"Today I decided to write up how to activate a Python 2 environment in Anaconda Python . I follow instructions from this website , but customize for my use case: I instal and run Python 2.7.18 where necessary. First, create a py2 Anaconda environment running Python 2.7.18 . Instead of using the very slow conda tool, use mamba instead! mamba create --name py2 python=2.7.18 In my case, the environment lives in ~/Applications/anaconda3/envs/py2 . Second, activate that environment and perform operations to install swig and HDF5 in it. conda activate py2 mamba install swig hdf5 Stuff should work!","tags":"computronics","url":"python-2-environments-for-anaconda.html","loc":"python-2-environments-for-anaconda.html"},{"title":"Jekyll as a blog on my github page subdirectory","text":"Jekyll looks like a really nice solution to build a blog, especially from Wordpress. However, the default tutorial assumes the MAIN PAGE is the blog. I don't want to do that. Instead, I want the blog to live in https://Tanim Islam.ddns.net/blog instead. Here's a HOWTO blog post , which I am not going to distill right now because I do not know enough about Jekyll to know what's what and why. Warning Instead, I am using Pelican with the Elegant theme to create my blog.","tags":"computronics","url":"jekyll-as-a-blog-on-my-github-page-subdirectory.html","loc":"jekyll-as-a-blog-on-my-github-page-subdirectory.html"},{"title":"Hippity-Hoppity SSH/SFTP/SCP","text":"Maybe you are frustrated like me. You have a file on server local , but you can't SCP it into server distant . However, at least you can scp it from local to near , and then near to distant . That's something...very very frustrating. Be frustrated no more, because this StackExchange article, How do I do Multihop SCP transfers? , explains how to do multi-hop SSH connections. Just edit your ~/.ssh/config SSH client configuration file by adding the following lines, Host distant ProxyCommand ssh near nc distant 22 Then you'll be able to SSH/SCP to/SFTP to host distant from local , even though you don't have a direct connection between them ! As a further aside, TRAMP mode in Emacs also allows hops! If you want to login to uname_distant@distant:<FILEPATH> on Emacs , using username and hostname of the near server uname_near@near , this is the syntax you should use. /ssh:uname_near@near|ssh:uname_distant@distant:<FILEPATH> And of course, use the standard C-x C-f to open/load the file on <FILEPATH> .","tags":"computronics","url":"hippity-hoppity-ssh-sftp-scp.html","loc":"hippity-hoppity-ssh-sftp-scp.html"},{"title":"Pretty useful necessary (but maybe not sufficient) process to play music from SSH + command line","text":"This article on Stack Exchange speaks directly to me. I run an Ubuntu 19.10 desktop server on which my music lives. I summarize. I have SSH'd into a machine I control, but which does not have a display active. I have speakers attached to this server. I have a monitor connected to my work laptop. My work laptop connects to the office through a VPN. If I connect my desktop to my office through a VPN, I destroy all the services I need to run on my server . I have tools like ffplay or mpv that, in a traditional login session (logging non-remotely into a new X Window session) can play music from the command line. I would like to run a command like ffplay -nodisp <MUSIC_FILE> and then have the speakers spit out sound. Apparently, this is what I needed to do. I operate under the assumption that this HOWTO will disappear at some unscheduled future date. Here is what I did, following advice from aforementioned article. Make a directory, ~/.pulse , if it does not already exist. Copy the default PulseAudio configuration file, cp /etc/pa.conf ~/.pulse/pa.conf . Edit ~/.pulse/pa.conf by creating the following line to make the PulseAudio server accessible to all users. # make PA accessible by all users load-module module-native-protocol-unix auth-anonymous=1 socket=/tmp/acpulsesocket Create a client configuration file, ~/.pulse/client.conf . Paste the following into ~/.pulse/client.conf , default-server = unix:/tmp/acpulsesocket Restart the PulseAudio server with pulseaudio -k . I haven't been able to get any sound out of my speakers, but at least my command-line players, like ffplay or mpv , aren't crashing! Warning Forget about this noise. Just perform an SSH tunnel connection like this, ssh -L 32400:localhost:32400 uname@local_ssh_plex_server And then, go to https://localhost:32400 to play music off the Plex server, through the Plex web client, that lives at local_ssh_plex_server .","tags":"computronics","url":"pretty-useful-necessary-but-maybe-not-sufficient-process-to-play-music-from-ssh-command-line.html","loc":"pretty-useful-necessary-but-maybe-not-sufficient-process-to-play-music-from-ssh-command-line.html"},{"title":"Location of iCloud Drive","text":"I followed instructions from this website. If that website goes down, iCloud drive lives in ~/Library/Mobile\\ Documents/com~apple~CloudDocs/ . This allows you to access stuff there from the command line, or make a symbolic link (I call mine iCloud ) in your home directory.","tags":"computronics","url":"location-of-icloud-drive.html","loc":"location-of-icloud-drive.html"},{"title":"Finally, instead of obliterating my branch, use \"theirs\"","text":"Thanks goodness for this link, which says, \"ignore what I did to my branch, just merge <their> branch.\" git merge -X theirs branchB Also, just run M-x server-start in Emacs to start the Emacs server . Then launch separate clients with emacsclient ; they will all share the same session.","tags":"computronics","url":"finally-instead-of-obliterating-my-branch-use-theirs.html","loc":"finally-instead-of-obliterating-my-branch-use-theirs.html"},{"title":"Try and remove youtube-dl cache when it 403s","text":"Today (18 April 2020), I noticed errors when using youtube-dl on Fiona Apple's new album, Fetch the Bolt Cutters . I was getting 403 errors that I couldn't diagnose; I had the most recent version of youtube-dl , my internet connectivity was fine, and I had not hit Google API's quota limits as described in this recent blog post . Fortunately, GitHub issue #24842 (NOW DELETED) for youtube-dl suggested a resolution. I removed my youtube-dl cache by doing rm -rf ~/.cache/youtube-dl . Everything was copacetic afterwards.","tags":"computronics","url":"try-and-remove-youtube-dl-cache-when-it-403s.html","loc":"try-and-remove-youtube-dl-cache-when-it-403s.html"},{"title":"Uncooperative screen modes: Ubuntu 18.04 in VM in Ubuntu 19.10","text":"I have an onion-like configuration for my work: My host machine is Ubuntu 19.10. My VM guest machine, running VirtualBox, is Ubuntu 18.04 In the VM guest machine, I also run a VNC connection This is an onion because, I run necessary services, many exposed as NGINX reverse-proxied microservices, on my host machine. I have to VPN into work. When I VPN into work on my host machine, ALL MY OUTGOING SERVER TRAFFIC IS BLOCKED -- no SSH, no HTTPS, no nothing... When I VirtualBox into my VM Guest machine, I get a nice 680x480 pixel screen. Changing the size of the VM GuestBox layer does not change the size of the underlying screen. Why the ever-loving fuck doesn't this fucking work? My host screen is 1920x1080. Why isn't my guest screen 1920x1080? I cobbled this together from the internet, some from this website I am not going to cite my sources, because the documentation is incomplete, bad when it is complete, and evanescent. Presumably it is possible to save the screen configuration. I could not because I could not cobble together a solution by starting at non-working suggestions. Here is what worked for me. Do everything from a command line shell within the guest VM (here, Ubuntu 18.04). cvt 1920 1080 . This is the screen resolution of my host machine, which I found by running xrandr | grep connected | grep -v disconnected . Running cvt 1920 1080 produces voodoo magic stuff-I-don't-understand but which you will need to copy when setting a valid screen mode: Modeline \"1920x1080\" 173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsync . Copy stuff starting with Modeline into an xrandr command: sudo xrandr --newmode \"1920x1080\" 173.00 1920 2048 2248 2576 1080 1083 1088 1120 -hsync +vsync . Figure out which display you're running. I run xrandr | head -n 5 and get this screen name, \"Virtual1\". Name of the screen in this case is \"Virtual1\" Add the new mode to screen \"Virtual1\" (or whatever your screen name may be) with this command, xrandr --addmode \"Virtual1\" 1920x1080 . Maybe these lines in your ~/.profile will work for you? Maybe not? xrandr --addmode \"Virtual1\" 1920x1080 xrandr --output Virtual1 --mode 1920x1080 Or not...","tags":"computronics","url":"uncooperative-screen-modes-ubuntu-18-04-in-vm-in-ubuntu-19-10.html","loc":"uncooperative-screen-modes-ubuntu-18-04-in-vm-in-ubuntu-19-10.html"},{"title":"Jupyterhub Reverse Proxy with Custom Prefix + Systemd Service","text":"I'm getting used to Jupyterhub , the recommended way to set up a Jupyter Notebook server from which you can log in everywhere; the standard way is to run jupyter notebook and spawn a local HTTP-like server on port 8000 (I think?). This has practical problems separate from port 8000 (this can be easily changed). What if you want to launch Jupyter notebooks from across the country, AND have access to the file system on which you've done all that work? Maybe run this as an user-based systemd service so that this daemon runs in the background? I prefer user-based services because I have a rickety contraption of persistent and one-shot (governed by timers) services. Root-level services should be much more stable. There are sets of instructions on how to install Jupyterhub on Ubuntu , on how to set up an NGINX reverse proxy to expose Jupyterhub to the rest of the world , and on how to create a Jupyterhub configuration file (just run jupyterhub --generate-config ). This blog post describes how I got my Jupyterhub setup to work. I want a Jupyterhub server that has the following properties. On localhost , this server runs on port 8080. To the external world, this server is accessible at https://<MY_IP_ADDRESS>/jupyterhub . A systemd unit file, jupyterhub.service , will initialize and run this server in the background from the server's session directory. Here are the specific steps needed to run a Jupyterhub server that is exposed to the outside world underneath your NGINX administered website. All the instructions live in a world in which you run Linux, and your Linux machine uses systemd to run services. Also, in the spirit of my rickety test services, I do everything as an user rather than root. First, install Jupyterhub as an user, following the installation instructions . python3 -m pip install --user jupyterhub Don't worry about having the wrong version of oauthlib -- I'm not going to bother trying to use OAuth2 authentication to the Jupyterhub server. Second, create a $CUSTOM_DIR in which the Jupyterhub server's session will live. cd into that directory. In that directory, generate the default configuration file. jupyterhub --create-config This will generate jupyterhub_config.py . This is a Python configuration file heavily commented with instructions. I only include the three lines here that tell it to run on port 8080 on the local machine, to have the mapping to /jupyterhub such that the NGINX reverse proxy works, and to launch Jupyter notebooks from Jupyterhub (see this Github comment ). c . JupyterHub . bind_url = 'http://127.0.0.1:8080/jupyterhub' c . JupyterHub . port = 8080 c . Spawner . cmd = [ '$HOME/.local/bin/jupyterhub-singleuser' ] These instructions began from the web page on how to set up a reverse proxy for Jupyterhub . Since you installed Jupyterhub as an user, the default location of jupyterhub and jupyterhub-singleuser is in $HOME/.local/bin . Third, make a change in your NGINX configuration file to reverse proxy your Jupyterhub server to https://<MY_IP_ADDRESS>/jupyterhub . Edit /etc/nginx/sites-available/default , which is the NGINX configuration file, with the following. These instructions also began from this section on NGINX reverse proxies . (3a) First, add this to the top, before the server block in the NGINX configuration file. # top-level http config for websocket headers # If Upgrade is defined, Connection = upgrade # If Upgrade is empty, Connection = close map $http_upgrade $connection_upgrade { default upgrade; '' close; } (3b) Second, add this code inside the server block in the NGINX configuration file. # jupyterhub reverse proxy # follow instructions from https://jupyterhub.readthedocs.io/en/stable/reference/config-proxy.html location /jupyterhub { proxy_pass http://127.0.0.1:8080; # port used right now proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # websocket headers proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection $connection_upgrade; } Check that the configuration file is kosher by running sudo nginx -t . If it's copacetic, you should see this. nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful Fourth, create an user service, jupyterhub.service , that lives in ~/.local/share/systemd/user -- the default location on Ubuntu for systemd user services and timers. To minimize confusion, here is an anonymized version of the service that I run. [Unit] Description=jupyterhub After=network-online.target [Service] ExecStart=$HOME/.local/bin/jupyterhub WorkingDirectory=$CUSTOM_DIR [Install] WantedBy=multi-user.target I follow instructions on running Jupyterhub as a systemd service . Furthermore, since you installed Jupyterhub as an user, the default location of jupyterhub is in $HOME/.local/bin . Here is how you'll know everything worked. First, on localhost , you should see this login page when you go to http://localhost:8080/jupyterhub . Second, you will see the same login page when you go to https://<MY_IP_ADDRESS>/jupyterhub . Third, once you login with your system username and password, you will be able to run Jupyter! Remotely too!","tags":"computronics","url":"jupyterhub-reverse-proxy-with-custom-prefix-systemd-service.html","loc":"jupyterhub-reverse-proxy-with-custom-prefix-systemd-service.html"},{"title":"Setting the title of an MKV file","text":"This blog post is a follow up to my previous post, Setting the audio track language in an MKV file . Here's how to do it using mkvpropedit , a command line tool provided under the mkvtoolnix distribution of MKV file editing and creation tools. mkvpropedit movie.mkv --edit info --set \"title=NEW TITLE\" Here movie.mkv is the name of the MKV file, and \"NEW TITLE\" is the title of the movie.","tags":"computronics","url":"setting-the-title-of-an-mkv-file.html","loc":"setting-the-title-of-an-mkv-file.html"},{"title":"My first Google API \"quota exceeded\"","text":"I think this was inevitable. I was trying to collect all the tracks in They Might Be Giants' album, Mink Car , released in 2001. To get all the tracks from this album by this artist, I use the CLI executable, howdy_music_songs , part of my GitHub project, howdy . I had been identifying and retrieving songs, track by track, when running this command from the shell, howdy_music_songs --artist=\"They Might Be Giants\" --album=\"Mink Car\" --musicbrainz Here, the flag --musicbrainz says to programmatically use the (more comprehensive and accurate, but much slower) MusicBrainz service to find those songs and populate their metadata. The song collecting was going OK until this part... ACTUAL ARTIST: They Might Be Giants ACTUAL ALBUM: Mink Car ACTUAL SONG: Older (01:57) WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"quotaExceeded\" Traceback (most recent call last): ... raise HttpError(resp, content, uri=self.uri) googleapiclient.errors.HttpError: <HttpError 403 when requesting https://www.googleapis.com/youtube/v3/search?q=They+Might+Be+Giants+Older&order=relevance&type=video&part=id%2Csnippet&maxResults=50&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\"> This is the first time I had hit my (free level) quotas on the Google API services (in this case, the YouTube API ) I use. So...hooray?","tags":"computronics","url":"my-first-google-api-quota-exceeded.html","loc":"my-first-google-api-quota-exceeded.html"},{"title":"ANUS","text":"Have you ever been watching TV, something OK like a freshness commercial, and BAM (AAAH)! Anus? Maybe you don't watch as much porn as I do? What if I told you there was a service that could tell you whether that thing you are about to see is an ANUS or not an ANUS? If that seems like something you heard, a popular TV show might have borrowed that idea from you-know-who. I've got a bunch of students. Students who are captive to the grades I can give them. I'm also a lazy professor, which means I gave my students a homework assignment: \"Each of you find me 2000 pictures of anuses and 2000 pictures of not-anuses for class next week.\" 20,000 distinct anuses and 20,000 distinct non-anuses later, I had something to test! That Community episode with the butthole on the school flag? ANUS. Star of David? NOT AN ANUS. now came the fun part: \"For your class projects, now that the ANUS/NOT-ANUS machine works, hook it up to whatever app or service that you want.\" You know, like an app that takes pictures and tells you if you took a picture of an ANUS. ((HEAVY SIGHING AND DROOPING DOWN)) I was really excited on project day. Someone hooked up the TV to the Google Assistant: have the TV tell you when an ANUS was about to appear on screen. We were all going to watch the original 1977 Star Wars: a space movie about a band of plucky terrorists who pick on a brave veteran too disabled to breathe without help. Surprisingly many kids in this technical course hadn't seen the original! ((HEAVY SIGHING AND DROOPING DOWN AGAIN)) Remember that scene where Grand Moff Tarkin is about to blow up Alderaan. Then that scene in the control room by the Death Star lasers. TV suddenly goes quiet. (( LOUD ROBOT VOICE -- ANUS )). TOTALLY (( EMOTE )) ruins the mood. Or that second scene. Death star is about to blow up the rebel base around Yavin. British guy says, \"You may fire when ready.\" TV suddenly goes quiet. (( LOUD ROBOT VOICE -- ANUS )). AAAGH! In hindsight, it turns out we should have put the STAR WARS IMPERIAL logo into our machine. So here I am. Turns out it's not too smart to yell ANUS in class. Especially when the dean of students can hear. Or Twitter.","tags":"book","url":"anus.html","loc":"anus.html"},{"title":"COPYRIGHT","text":"I want to start with a big shout out to Alexander McAllister on running this thing every Monday. I can't imagine how hard it's been. I run a happy hour at the lab where I work, but only when I feel like it. And it doesn't matter if no one else shows up. Getting people every week, and persuading them to pay for things, is a lot harder. I confess that I am a hard-core Communist. In my perfect world, I don't think anyone should pay anything for anything, at any time, under any circumstances. In real life, that means I haven't paid for a song or a TV show or a movie online in over two years. For me BitTorrent (for TV and movies) and Youtube for music have been great ways to make that happen. I had been doing all this stuff for over 15 years — The Simpsons was one of the first things I used Bittorrent to get — but doing it all by hand was so clunky, and watching it on my laptop or on the TV took so many steps I would sometimes quit halfway through. That all changed with Plex. For those who don't know what Plex is, it's just like Roku. It's just like Netflix or Hulu or Amazon Prime, except you don't pay to use it. This meant that instead of putting a TV show or movie onto a thumb drive and sticking it on my DVD player and then clicking the DVD remote and then clicking the TV remote…(lots of steps I know), I could use my iPhone as a remote and watch straight on the TV. Everything I have, organized and indexed better than any paid service, for free. Also, my friends could watch everything I had, same as I could. Plex made it look like I could do so much more with getting free movies, TV shows, and songs than I had done before. I'm lazy about some things and impatient about others. There are all these free and big applications you can put on your computer to get you free media. But they were hard to use (I'm lazy and impatient). they didn't do what I wanted to do exactly howI wanted to do it (again, lazy), and they crashed the computer where I kept all my stuff. So I thought, why not write my own in Python? I started off with a few things — write a script to take the 10 steps to get a TV show I wanted onto my computer, into say 5; or automatically look for songs on YouTube. Like all hobby programming projects I started, it grew (Python is great for this, by the way). I started having it do more and more things, until the past few months I ended up with this. I gave it a name (giving all this work a name makes it harder to walk away). I ended up with 25 apps to download and organize TV shows, movies, and songs I want to get, and the Plex server that lets me and my friends watch whatever they want to watch when they want to watch it. I put it on Github, and put all the documentation near it. Since I'm not too much into hypocrisy, all that stuff I wrote is sitting there without copyright. Someone can copy it and then try to take me down with a copyright infringement claim. Now if I want to… automatically get new TV shows every day, or a friend wants a new TV show or movie to watch, or get all the songs made by an artist I heard on Shazam, or go down a rabbit hole of movies, like other movies, like other movies, to download I spend all of 60 seconds to make it happen. Python is awesome! In the back of my mind, I've wondered if I should quit my job and go down the road to Plex in San Mateo with a more polished version of this here job talk. It could end with an offer, or it could (more likely) end with a knock on the door from the FBI! I tell myself, I'm fine either way. Thanks everyone!","tags":"book","url":"copyright.html","loc":"copyright.html"},{"title":"MENTHOLS","text":"Too long; didn't read: don't put anything in a mailing list email that you wouldn't want middle America to read. The long story goes something like this instead. I am nominally the head of the new hires social group at my place of employment. In practice, I arrange a semi-regular meeting of lab new hires and postdocs at a wine bar in Livermore on Thursday evenings. My thing used to be having snarky announcements with misleading subject lines and photos, like: \"Management positions for everyone!\" \"insert pithy statement here.\" \"I once knew a guy...you look like 'im...but he wasn't one either.\" Of course, all this ended with this email, which comes from a poster I saw my best friend put up on his dorm room door. I sent an announcement for a new hires event with the subject line, \"MM: Got Any Ciggies? JC: Sorry, only Menthols,\" at an airport on my way back to Richmond, Virginia, for Christmas break. And then promptly forgot about this email. UH-OH! During the next day and a half after my trip when I was relaxing in Hopewell, Virginia, a conversation was brewing on the new-hires mailing list. My friends were pretty sympathetic, saying things like: \"Lol, poor Tanim playing with Christian fire. Looks like you were crucified for this one, buddy!\" \"Oh man, now you've done it...Incidentally, it's probably good you didn't use Raptor Jesus . \"OMG, he could have just told you. He didn't have to spam the rest of us. Eye roll.\" Others thought the matter could have been handled differently: \" ...I'm a Christian too! What happened to peace on earth and goodwill to men? If the gates of hell can't prevail against our religion, I think Tanim's wacky humor won't do too much damage.\" \"I am sorry you had to deal with this very unprofessional response (his concern is valid; his response is uncalled for)...I hope you get this resolved without too much headache.\" \"...On the other hand, I really believe Islam's email was by no means intentional. Somehow it didn't occur to him (or her) at the moment that this could hurt other people's feelings...\" Yet others were very very displeased: \"...Regardless of any intended humor, the picture itself is quite offensive. It is also rather ironic, given that your name-sake religion expressly forbids any visual representation of Christ (or any other prophet)...\" \"I don't generally like to make waves, but I have to point out that I also found the image you included in your e-mail to be disrespectful. ...\" And then there was one person angry enough to report me to HR: \" The subject line and picture in this email are completely inappropriate and unacceptable. As a Christian I am horribly offended that you feel it is ok to mock Jesus whom I and many others consider to be the Son of God and most important part of our spiritual life. This email is horribly disrespectful and not ok! Certainly it not appropriate for mass distribution in a work environment. I've CC'd <person> of the Employee Relations Office in the hopes that they might contact your management and have a discussion with you about how it is not ok to make fun of other peoples religion. \" Meanwhile, I get a cell phone call while I'm Christmas shopping at Colonial Heights Mall with my parents two days later. I don't recognize the phone number. Hello, I say? It's my group leader (my immediate boss)! He asks me if I've checked my emails, that I've pissed off a fair number of people, and that I am supposed to send HIM an apology letter that he will vet before I send it off to the mailing list. This is what we quickly agreed on, \" To all workers, A few days ago, I sent out an announcement that was, in obvious hindsight, in shockingly poor taste. To the entire combined lab community, I wholeheartedly apologize for my very poor judgement in sending this out this email. I apologize for the shock, hurt, and anger that I have caused our community. I apologize for betraying the implicit and explicit trust that you have given me as one of your members. I will strive to personally apologize, by phone or by person, those people who have taken the time to respond to my very inappropriate action. I can guarantee that I will not repeat this, or any similar incident, again. Tanim Islam \" But there's more! After my apologetic email, within my first hour after coming back to the lab after vacation, I had to show up to HR's main office with my group leader. Two things stuck in my mind: HR basically said there's a file on me, this incident went up to the Director's office, and that I am on permanent notice if anything like this happens again. My immediate boss asked me, \"Why did you think this cartoon was funny?\" How could I answer the first question honestly, considering that once upon a time I sold an eighth of oregano to a buyer as a joke? Anyhoo, I think I've learned my lesson. At least I didn't send this in an announcement: Or this: Or this: So remember ( AGAIN ), don't send out a corporate email that you wouldn't want Middle America to read!","tags":"book","url":"menthols.html","loc":"menthols.html"},{"title":"Setting the audio track language in an MKV file","text":"I was never able to find instructions on how to set the audio track of an MKV file to ENGLISH, until I found this article on the Plex forums . Here's how to do it using mkvpropedit , a command line tool provided under the mkvtoolnix distribution of MKV file editing and creation tools. mkvpropedit movie.mkv --edit track:a1 --set language=eng --edit track:v1 --set language=eng Here, movie.mkv is the name of the MKV file and eng is the target language, ENGLISH.","tags":"computronics","url":"setting-the-audio-track-language-in-an-mkv-file.html","loc":"setting-the-audio-track-language-in-an-mkv-file.html"},{"title":"How to encode from a DVD into an MKV using HandBrakeCLI","text":"Every so often I try to encode a video located as a title in a DVD, and every time I have to scour the internet. Instead of trying to reproduce my steps each time, I am going to put these instructions here so I will know immediately how to do so. The name of the DVD file is EXAMPLE_DVD.iso , and I will be using HandBrakeCLI . First, find out the list of titles on this DVD, run this command, HandBrakeCLI -i EXAMPLE_DVD.iso -t 0 >& foo.txt This will get you a nicely formatted list of DVD titles with information, such as length of the title. Here is what HandBrakeCLI 's output will look like, + title 7: + Main Feature + vts 7, ttn 1, cells 0->11 (965183 blocks) + duration: 00:46:04 + size: 720x480, pixel aspect: 8/9, display aspect: 1.33, 29.970 fps + autocrop: 58/62/6/6 + chapters: + 1: cells 0->0, 91569 blocks, duration 00:04:44 + 2: cells 1->1, 62934 blocks, duration 00:02:49 + 3: cells 2->2, 58200 blocks, duration 00:02:41 + 4: cells 3->3, 96179 blocks, duration 00:05:12 + 5: cells 4->4, 72236 blocks, duration 00:03:14 + 6: cells 5->5, 64838 blocks, duration 00:02:55 + 7: cells 6->6, 54341 blocks, duration 00:02:31 + 8: cells 7->7, 94486 blocks, duration 00:04:04 + 9: cells 8->8, 110402 blocks, duration 00:05:00 + 10: cells 9->9, 128666 blocks, duration 00:06:08 + 11: cells 10->10, 125403 blocks, duration 00:06:03 + 12: cells 11->11, 5929 blocks, duration 00:00:45 + audio tracks: + 1, English (AC3) (2.0 ch) (iso639-2: eng), 48000Hz, 192000bps + 2, English (AC3) (2.0 ch) (iso639-2: eng), 48000Hz, 192000bps + subtitle tracks: + 1, English, Closed Caption [CC608] Second, now encode this into an MKV file. Since the titles in the DVD are in mpeg2video video encoding, choose an encoding setting with a quality of 20. Here, the output file is movie.mkv , and it is title #7. HandBrakeCLI -i EXAMPLE_DVD.iso -t 7 -e x264 -q 20 -B 160 -s 1 -o movie.mkv Instructions on using other settings for the video file ( H264 encoding, 160 kbps, passing through subtitles) can be found on the HandBrakeCLI instruction website .","tags":"computronics","url":"how-to-encode-from-a-dvd-into-an-mkv-using-handbrakecli.html","loc":"how-to-encode-from-a-dvd-into-an-mkv-using-handbrakecli.html"},{"title":"How to use git-filter-repo to change inconvenient emails","text":"I found that I wanted to replace inconvenient emails in the git commits in my Github project, specifically howdy . I wanted to remove old email addresses and use the default one -- Tanim Islam <tanim.islam@gmail.com> . On the #git channel on Freenode I was pointed into the right direction. Here is what I needed to do to get it to work. First clone git-filter-repo from Github. Copy the executable, git-filter-repo , into some place on your $PATH. To work, git-filter-repo requires at least git 2.24. On Ubuntu 19.10, I only had git 2.20. To get the latest version, I had to do the following. Get the latest stable Ubuntu Git PPA package by running, sudo add-apt-repository ppa:git-core/ppa Install by running, sudo apt update && sudo apt upgrade . Construct a mailmap file with the single line: Tanim Islam <tanim.islam@gmail.com> Tanim Islam <OLD_EMAIL> . Now here is what I needed to do, all from the git repository, where in one local copy I replace <OLD_EMAIL> with tanim.islam@gmail.com . Run git filter-repo --mailmap mailmap --force . This replaces all commits on the local copy, whose email addresses are <OLD_EMAIL> , with tanim.islam@gmail.com . The --force is necessary because I was running from a non-fresh checkout. After it is done, you notice that references to the remote branches disappear (in my case, howdy on Github). Add the remote branch with this command, git remote add origin https://github.com/Tanim Islam/howdy.git . Finally, obliterate whatever exists on Github by running git push --set-upstream origin master --force . To be extra careful, run these two commands to ensure that the email remains tanim.islam@gmail.com and that the name is \"Tanim Islam\", run these commands. git config --global user.name \"Tanim Islam\" git config --global user.email \"tanim.islam@gmail.com\" On other local copies on another machines, I would like to obliterate the history by replacing <OLD_EMAIL> with tanim.islam@gmail.com . Here is what I did for each copy on different machines, following advice given on this Stackoverflow post . git fetch --all git reset --hard origin/master git pull I confirmed that the commit history does not have any <OLD_EMAIL> by running git log grep OLD_EMAIL , which returned NO lines. On each machine, for each copy of the repository, I ensured that future emails would be tanim.islam@gmail.com with these two commands that I run within a checked out copy of the repo. git config user.name \"Tanim Islam\" git config user.email \"tanim.islam@gmail.com\"","tags":"computronics","url":"how-to-use-git-filter-repo-to-change-inconvenient-emails.html","loc":"how-to-use-git-filter-repo-to-change-inconvenient-emails.html"},{"title":"Reached a milestone in my hobby project","text":"I am happy that I have reached a milestone in howdy (formally plexstuff ), a hobby project on which I have been working since 2015 (but whose development has really picked up since the end of 2017). I have finished the documentation for this SDK, located at https://howdy.readthedocs.io for the non-graphical API of the separate submodules and all but one of the CLI (except for plex_email_notif.py ). There are now YouTube clips of four demonstration movies on my channel. I am also pleasantly surprised that straightforward video editing I did for the project's documentation (choosing a window over which to do a screen capture video of a CLI, and trimming the clip's start and end times) is EASIER and results in smaller file sizes than what I had done on Mac OS X. I have to thank SimpleScreenRecorder for screen capture, and LosslessCut for video clip trimming of start and end time, to make the screen capture videos on my YouTube channel so painless. I was able to install SimpleScreenRecorder by running the following on my Ubuntu 19.10 desktop, sudo add-apt-repository ppa:maarten-baert/simplescreenrecorder sudo apt update sudo apt install simplescreenrecorder I was able to install LosslessCut from the project's download page .","tags":"computronics","url":"reached-a-milestone-in-my-hobby-project.html","loc":"reached-a-milestone-in-my-hobby-project.html"},{"title":"How to turn off automatic smart invert","text":"For quite a while, I would trigger smart invert on my iPad Air 2 when I accidentally triple-clicked my iPad's home button. That was very annoying! I would have to go to General → Accessibility → Display & Brightness, and then toggle off the Smart Invert button. It was very annoying! Instead, I found out that I could just turn off the shortcut that I probably inadvertently set, where triple-clicking the home button turns on smart invert. Just go to Settings → Accessibility → Accessibility Shortcut, and uncheck Smart Invert Colors. And now you're good to go!","tags":"computronics","url":"how-to-turn-off-automatic-smart-invert.html","loc":"how-to-turn-off-automatic-smart-invert.html"},{"title":"Marijuana","text":"Question, everyone. Show of hands: who here has used marijuana < I raise my hands > (don't worry, you don't have to raise your hands)? Another show of hands: who here has ripped someone off who was trying to buy marijuana < I raise my hands again >? I've told a version of this story to friends after college, for my original lit to the Washington Society, to coworkers at a workshop after a bottle of wine, to my federal investigator during my initial security clearance interview. I've cleaned up some of the language because we're all living in times more enlightened than when I was still growing up, and I should know better. Seven months before the story, it's my 21st birthday. For many, it's drinking too much and sometimes waking up not knowing where you got there. If you're white, you're probably waking up at Panera Bread. My 21st was a bit different: my roommate and friend decided that marijuana was the best birthday present I could get. He was right! $300 in three full bags of marijuana makes me happy and popular! Eating the rest of the evidence on my flight back to Richmond a few days later gave me the calmest plane ride I've ever had! Anyways, I got an email out of the blue from Some guy on early Sunday morning, April 9, 2000. I guess he heard about my birthday party. It went something like this. Hey,I hear you're a stoner now...Think you could hook me and my roommate up? Some guy I then replied to him something ridiculous. I didn't actually think he would actually think I was serious, and that I had some weed. But oh well... Yeah, sure. Come by at 2 PM on Sunday, this Sunday. I will have so much weed that you'll fart out your ears. Tanim Then some guy took my message seriously. What presumption — even though I was a pothead, I don't want some rando to peg me as one and ask for weed out of the blue! At first, I was angry at him, showing my email correspondence with Some guy to my roommate at the time, as a joke suggested that I give him oregano as \"weed.\" But then, I thought, what a good idea! So he and I went to that organic supermarket on California Blvd, where we got one oregano shaker and a small pack of ziplock bags, all for ~$5. I sent the following message shown below, just before my roommate and I left for the supermarket: Come by this Sunday, 2 PM, at the Ruddock library. > Cool, but where are you? Also, I won't have time to get too high on > Sunday cause I have two problem sets due tomorrow, but can I buy some to take home with me? > > Some guy We started a really stupid email exchange, because obviously Some guy was very nervous about getting caught. Yeah, sure, you can buy some. It's sitting around here right now, waiting for you. Just reply if you got this email. Okay, I'll come by at 2 to buy some. Still want to meet at the library right? Some guy -- Yeah. The library. Let's not email anymore. People might think we're XXXX <make grunting sounds>. Tanim Some guy arrived with his roommate, pretty much on time. The two who were \"busy with two problem sets\" were really \"fiending\" for some weed, among other things. Well, right before he arrived, my roommate and I noticed that the \"eighth\" of \"weed\" really smelled like oregano. One bag wasn't enough to cover the smell, so we scrounged for an extra ziplock bag to cover it up. There were many flaws in our plan. Plus, a group of people from both floors were congregating...some thought some guy wouldn't be stupid as to forget checking if the weed was real. I was getting kinda nervous -- I didn't want a scene. As I mentioned, I had a reputation to keep as a \"drug dealer\" who operates out of the Ruddock Library, in plain view of everyone. Some guy and his friend arrived. He asked in a very low voice if I \"had the weed on me.\" Well, I just got an eighth of weed out of my pocket, and he quickly snatched the weed into his pocket. Remember, people were studying in the library while the drug deal was going down. He gave me the $25 I asked. But then he said that his roommate could pay up to $75 for his drugs, and some guy could pay about $70, for something like $145 for both. We then walked by my room, where he asked -- he practically begged -- for more drugs. He asked if he could have more weed, up to the $145 for more drugs, like LSD. I told him that I don't really know what kind of weed I had, that my roommate the dealer hooked me up with whatever he had lying around. I told him that right now we had Red Sasparilla and Green Firecracker -- it didn't matter since I was talking out of my butt. My marijuana habit, up to the time I quit, was called \"smoke whatever your friends can find.\" I also told him that I didn't carry LSD or Ecstasy \"right now,\" but something like \"your comments are important to me, and I'll make sure I get you the drugs you so definitely need and deserve, as expeditiously as possible.\" Of course, I wasn't that blatant, what with being afraid that he'd trip me up and catch me. I was surprised he didn't notice the weed was double-bagged! In fact, we walked pretty slowly out of Ruddock, him talking in a low voice, my talking a little louder than usual. He really, really, really wanted his LSD, crank, weed, Ecstasy, and what-not. I wished him well, saying that I \"...probably wouldn't have any new stuff by Wednesday, but I'm sure I'd get something by Saturday at the latest.\" Also, the stuff I got would be \"whatever my dealer had on hand, so no promises.\" Of course, my plan was (if he was stupid enough) was that NEXT weekend my DEALER (or, my roommate) would \"transact\" the drug business, for anything up to and including 150 dollars. Here, some guy and friend get angry. To me, he says the following, about 30 minutes after the deal went down. What the fuck is this shit? Then after talking about the incident with everyone around me, I decided to treat myself and the mutual friend to Baja Fresh. Walking out of the Ruddock parking lot, one of my friends rushed up to me. Some guy is also a friend of my friend. To my friend, he has this gigantic online chat (or diatribe, I can't tell which), about how I'm so sketchy, so scummy, and how he'll report me to the police and/or the Board of Control, my college's version of the Honor Council (about how I took \"advantage\" of him, or something like that). But the funniest thing, that thing I will remember for as long as I live, is that he said: ...I smoked something like FOUR BOWLS of this stuff, and either this shit is oregano or the weakest fucking weed in the history of the world. I guess he didn't notice that the SMELL of the weed is overpowering, like OREGANO, or that his weed TASTES like OREGANO. Yet another friend of mine suggested this reply; I can't take credit, but a good idea is a good idea! What the fuck are you talkin' about? I got high as a mutherfuck off that shit!! And don't blame me, man, I get it from somewhere else. Hey, but anyways, you should dip this in liquid Drano and roll it up in toilet paper. I've never done this, but I hear that shit will really fuck you up. Peace, Brutha","tags":"book","url":"marijuana.html","loc":"marijuana.html"},{"title":"My Philosopy on Driving","text":"This consists of notes, that I fleshed out (with animations! with pictures!) of my driving philosophy. REAL TITLE : \"Off the highway, and out of the country, you should be driving 5-10 miles per hour under the speed limit\". As you all may know, a bunch of lazy justifications and illogical and kind of circular rules turn this philosophy, or mission statement, into a religion. The only thing it's missing are followers and an official bible. Lots of people who aren't paying attention (like yourself), and lots of traffic lights randomly turn red and stay that way. Why hurry up and wait -- the red light isn't getting any greener? And why give yourself less time to anticipate accidents? Why ruin your car's brakes or waste energy (if you don't have a hybrid or electric)? Why stress yourself at all? Here's a few things that work great for me. Cruise 5 MPH below the speed limit in normal traffic, aim for 10 MPH during rush hour. In a traffic jam, say the car in front of you is stopping and going; wait 10 seconds before it starts moving, and just take your foot off the brakes. Coast! Don't put on the gas until traffic starts moving 20 MPH or faster for 30 seconds or longer. If the light in front of you turns red, take your foot off the gas. If you're going to turn into another road, right or left, coast (even with a green light) so that you roll into the street at 13 MPH. 13 MPH is the fastest you can take turns without wearing your tires. What about hills? I'm talking about this some more since this is the newest chapter in my bible. Going up hills are an opportunity for gravity to do your work for you (so to speak): light's red, you're going up, so cruise slower and coast earlier. Time your speed and your FOOT OFF THE PEDAL so you roll to a stop at the red light. Bonus points if you're going 10 MPH when the light goes green. If you're going up a hill, tap your brakes 5 feet vertically from the top and coast through. Aim to go 5 MPH below the speed limit at the bottom of the hill. If there's a red light at the bottom of the hill, just put your feet on the brakes so you slow constantly to a stop at the light. If it's green and you're taking a turn, slow constantly so that you're going 13 MPH through it. Make it fun and habit forming for you! ( Slow driving is just curling! ) The lights in front are probably going to turn red soon, if they're green. Why rush by putting your foot on the gas? Just coast if you feel it. See how close you can get to the red light before it turns green, or try to get to the red light JUST as it turns green. Let gravity and friction do the work for you! And before you just say no, see if you can answer this: you're trying this out, some driver goes in front of you, and there's that red light. If they end up the same place you did, except they hurried to get there, why not keep on keeping on? Or at least until you get shot. THE MATH These notes come from the 10-minute-long Washington Society literary presentation I gave on 24 April 2020, colloquially titled, \"Off the highway, you should drive 5-10 MPH below the speed limit.\" One of the main points in my talk was that you should aim for speeds of about 13 MPH when turning left or right onto a road at 90 degrees from yours, in order to get the maximum acceleration that will keep your tires from wear. I soon realized that the turning radius is approximately 30 feet. If we round up to 15 MPH [1] , then the acceleration here is, \\begin{equation*} a_{\\text{canonical}} = \\frac{ v_0&#94;2}{r_0} \\approx 4.92\\text{ m/s$&#94;2$} \\approx 0.5\\text{ g}. \\end{equation*} Here, \\(v_0 = 15\\text{ mph} \\approx 6.71\\text{ m/s}\\) , and \\(r_0 = 30\\text{ ft} \\approx 9.1\\text{ m}\\) . One can then make this canonical acceleration and deceleration when controlling your car: acceleration from stop, deceleration to stop sign or turning road, and so on. I want to explore accelerating from rest, into a 90 degree turn, over the 30 foot turning radius, such that the total acceleration stays at \\(a_0\\) . Geometry is below in problem depiction , Cartoon showing the geometry of the car's motion along the circle from a stop, through a turn of radius \\(r_0\\) , and a constant total acceleration of \\(a_0\\) . Since the car starts from rest, at \\(\\theta = 0\\) , the angular velocity \\(\\dot{\\theta} = 0\\) . The canonical equation of acceleration is, \\begin{equation*} r_0\\sqrt{\\ddot{\\theta}&#94;2 + \\dot{\\theta}&#94;4} = v_0&#94;2 / r_0. \\end{equation*} If we make the following variable changes, \\begin{equation*} \\tau = \\frac{v_0 t}{r_0}, \\end{equation*} \\begin{equation*} \\dot{\\theta} = \\frac{v_0}{r_0} y, \\end{equation*} \\begin{equation*} \\ddot{\\theta} = \\left(\\frac{v_0}{r_0}\\right)&#94;2 \\left(\\frac{dy}{d\\tau}\\right). \\end{equation*} Then the canonical acceleration relation reduces to, \\begin{equation*} \\left( \\frac{dy}{d\\tau} \\right)&#94;2 + y&#94;4 = 1. \\end{equation*} with the condition that \\(y(0) = 0\\) . Note that \\(y = v/v_0\\) , where \\(v\\) is the car speed; and the normalized transverse acceleration \\(a_{\\text{trans}}\\) is, \\begin{equation*} a_{\\text{trans}} = \\left( di{y}{\\tau} \\right) \\frac{v_0&#94;2}{r_0} = \\frac{v_0&#94;2}{r_0}\\left( 1 - \\frac{v&#94;4}{v_0&#94;4} \\right). \\end{equation*} The solution to the the master relation comes from Wolfram Alpha , \\begin{equation*} y\\times \\mbox{}_2F_1\\left( \\frac{1}{4},\\frac{1}{2}; \\frac{5}{4}, y&#94;4 \\right) = \\tau. \\end{equation*} The angle that is traversed as a function of time is, \\begin{equation*} \\theta(t) = \\int_0&#94;t \\dot{\\theta}\\left( t' \\right)\\,dt' = \\int_0&#94;{\\tau}\\frac{v_0}{r_0} y\\left( \\tau' \\right) \\frac{r_0}{v_0}\\,d\\tau' = \\int_0&#94;{\\tau} y \\left( \\tau' \\right)\\,d\\tau'. \\end{equation*} After more Wolfram Alpha , one can then show that the angle is related to the scaled speed \\(y = v/v_0\\) by this relation, \\begin{equation*} \\theta = \\sin&#94;{-1}\\left( \\frac{1}{2}v&#94;2 / v_0&#94;2 \\right). \\end{equation*} The speed \\(v\\) versus scaled time \\(\\tau\\) is shown in this illustrative figure . On the left is scaled speed versus scaled time, and on the right is scaled transverse acceleration versus scaled time. The angle, in degrees, along its motion as a function of time is shown in another illustrative figure . Position, in degrees, along the curve as a function of scaled time, \\(\\approx 1.311\\) (see the relation on scaled maximum time ). Here are the properties of this car's motion, in general and specific to my definition of \"safe driving\" (constant speed of 15 MPH through a 30 FT turning radius). Its maximum speed, \\(v_{\\text{max}} = v_0\\) ; in my case, 15 MPH. It reaches its maximum speed at \\(t_{\\text{max}}\\) , given by, \\begin{equation*} t_{\\text{max}} = \\frac{\\sqrt{\\pi} \\Gamma( 5/4 )}{\\Gamma( 3/4 )} r_0 / v_0 \\approx 1.311 r_0 / v_0. \\end{equation*} In my case, this is approximately 1 second later. It reaches this maximum speed at an angle of 30 degrees along its motion, \\(\\theta_{\\text{max}} = \\pi/6\\) , see the theta relation , or in my case approximately 16 feet into the circle. In my case, start accelerating at 0.5 g, or when looking at the speedometer, a change in speed of \\(\\approx 11\\) MPH/second. I continue to move at a speed \\(v_0\\) along that circle for 90 degrees, until moving into the perpendicular street. [1] I move at 15 MPH because it is easier to gauge this speed on my odometer.","tags":"book","url":"my-philosophy-on-driving.html","loc":"my-philosophy-on-driving.html"},{"title":"Cancelling Ultraseedbox account","text":"I wanted to cancel my Ultraseedbox account. I could find no instructions that told me how to put a cancellation request from there . Fortunately, the community on the Ultraseedbox Discord server , specifically in the #general chat room, was very helpful in showing me how to cancel my service. I summarize below. Go to your client page, and click on services. Choose the service to cancel. In my case, I chose to cancel my basic seedbox servce. Select \"immediate cancel\" on service. You have to provide a reason. In the end, you should see this final page showing that your service has been cancelled. If this doesn't work, then I cannot help you!","tags":"computronics","url":"cancelling-ultraseedbox-account.html","loc":"cancelling-ultraseedbox-account.html"},{"title":"Getting plex_music_songs.py to upload multiple songs","text":"these instructions are deprecated! I spent a little bit of time wondering why plex_music_songs.py and the associated backend for emailing functionality, plexemail/plexemail.py , would not work to upload multiple songs to people's accounts. The whole help syntax for plex_music_songs.py is, Usage: plex_music_songs.py [options] Options: -h, --help show this help message and exit -s SONG_NAMES, --songs=SONG_NAMES Names of the song to put into M4A files. Separated by ; -a ARTIST_NAME, --artist=ARTIST_NAME Name of the artist to put into the M4A file. --maxnum=MAXNUM Number of YouTube video choices to choose for each of your songs.Default is 10. -A ALBUM_NAME, --album=ALBUM_NAME If defined, then use ALBUM information to get all the songs in order from the album. -e EMAIL, --email=EMAIL If defined with an email address, will email these songs to the recipient with that email address. -n EMAIL_NAME, --ename=EMAIL_NAME Only works if --email is defined. Optional argument to include the name of the recipient. --new If chosen, use the new format for getting the song list. Instead of -a or --artist, will look for --artists. Each artist is separated by a ';'. --artists=ARTIST_NAMES List of artists. Each artist is separated by a ';'. --lastfm If chosen, then only use the LastFM API to get song metadata. The way to upload multiple songs, say from multiple artists, was fairly straightforward. plex_music_songs.py --new --artists=\"Ximena Sariñana;Natalia Lafourcade\" -s \"Different;Piensa en Mí\" -e <email_address> -n \"Tanim Islam\" Artists are separated by \";\", songs are separated by \";\", and the songs this executable downloads and emails are, Different by Ximena Sariñana . Piensa en Mí by Natalia Lafourcade . I had to construct a sendmail relay through GMail's SMTP server to allow me to attach multiple songs together in an email. The Sendmail Relay Setup and Implementation blog post describes the setup and implementation. The SMTP code blog post describes the Python implementation I use in code to send the email through GMail.","tags":"computronics","url":"getting-plex_music_songs-py-to-upload-multiple-songs.html","loc":"getting-plex_music_songs-py-to-upload-multiple-songs.html"},{"title":"Using StackEdit to Create Pretty Blog Posts","text":"I found that StackEdit is a great way to create blog posts for my blog in the Markdown markup language. An example blog post I generated with StackEdit is Getting plex_music_songs.py to upload multiple songs . Warning This blog post refers to tools and blogs that no longer exist!","tags":"computronics","url":"using-stackedit-to-create-pretty-blog-posts.html","loc":"using-stackedit-to-create-pretty-blog-posts.html"},{"title":"Python Code to Send Email Locally","text":"Here is the block of code I needed to send an email in Python from my local host machine. I use port 25, the standard port for sending email. import smtplib smtp_conn = smtplib . SMTP ( 'localhost' , 25 ) smtp_conn . ehlo ( 'test' ) smtp_conn . sendmail ( msg [ 'From' ], [ msg [ 'To' ], ], msg . as_string ( ) ) smtp_conn . quit ( ) smtplib is the Python implementation of a Sendmail client. msg is an instance of a MIMEMultiPart Python object -- a MIME email with possible attachments.","tags":"computronics","url":"python-code-to-send-email-locally.html","loc":"python-code-to-send-email-locally.html"},{"title":"Sendmail Relay Setup and Implementation","text":"I follow instructions from this website . One also needs to set up your Google account for two-factor authentication, which that website does not fully articulate. Here are some changes and extra steps needed to get email relay with large emails with attachments to work properly. In my case, I set up the fully qualified domain name (FQDN) of my Postfix server to tanimislam.ddns.net . In the website's section, Add Gmail Username and Password to Postfix , one has to use the app password set up for the Postfix server that Google generates. The website does not explicitly state that. Finally, one has to set up Postfix to allow large emails to be sent from your local host's Sendmail server. I followed advice from this other website , running, sudo postconf -e message_size_limit=102400000 This limits the size of emails to be less than \\(1.024\\times 10&#94;8\\) bytes. On Ubuntu, restart the Postfix server with sudo service postfix restart to get the new configuration. On later versions of Ubuntu, such as 19.04 , restart the Postfix server with sudo systemctl restart postfix to get the new configuration.","tags":"computronics","url":"sendmail-relay-setup-and-implementation.html","loc":"sendmail-relay-setup-and-implementation.html"},{"title":"ffprobe to get output in JSON format","text":"I keep going to this Stackoverflow post that describes how to use ffprobe to return output information, on a media file, in JSON format. Instead of constantly returning to that article (which may disappear), I post instructions below, /usr/bin/ffprobe -v quiet -show_streams -show_format -print_format json FILENAME","tags":"computronics","url":"ffprobe-to-get-output-in-json-format.html","loc":"ffprobe-to-get-output-in-json-format.html"},{"title":"More fun with the Plex Media Server, downgrading helped","text":"I had more recent fun with the latest version of the Plex Media Server -- specifically, 1.10.1.4602-f54242b6b on Ubuntu 64-bit Linux . When I restarted my Ubuntu Linux box on which the Plex server resides, I could not telnet to localhost port 32400. However, when I downgraded to an older version of Plex , 1.9.3.4290-9798172d4 on Ubuntu 64-bit Linux , everything worked again. The Plex runs but does not open a connection to port 32400 seems like a bug that the Plex folks do a poor job addressing. I don't have an answer to that. If I inadvertantly upgrade my Plex media server into something that doesn't work, I have put the 1.9.3.4290-9798172d4 on Ubuntu 64-bit Linux deb file into my Dropbox account, under the plexfails directory.","tags":"computronics","url":"more-fun-with-the-plex-media-server-downgrading-helped.html","loc":"more-fun-with-the-plex-media-server-downgrading-helped.html"},{"title":"How to access documents in CENLAR mortgage account","text":"CENLAR is an incompetent mortgage company. Their customer service is terrible. However, I am forced to be their customer since I cannot yet pay off my mortgage loan. I sometimes need to access my loan and tax documents. Here is how to do that. Go to the main portal website , and click on the \"Manage Your Account\" square. Click on the \"Statements and Documents\" menu item to populate that menu. Then click on the \"Document Center\" menu item to access your documents. CENLAR will then take you to a crappy, third-party website that holds your documents. I make no guarantees on how long their crappy, undocumented procedure for accessing your loan related documents will work.","tags":"computronics","url":"how-to-access-documents-in-cenlar-mortgage-account.html","loc":"how-to-access-documents-in-cenlar-mortgage-account.html"},{"title":"My Plex media server did not analyze all the files in my TV Show media library","text":"When I was at my parents' house for Christmas break, I used my Plex python SDK, plexstuff , to determine which episodes I did not have. Strangely, my check gave me lots of false positives: episodes that my functionality told me were missing, where the Plex server said they were available. Over a week ago (31 December 2017), I skimmed through the relevant python code to determine why the SDK showed missing episodes. I eventually learned that, although Plex showed that those media files existed, it had not analyzed those \"missing\" media files. That is, Plex had not determined their sizes, durations, or file types. My code ignored media when Plex could not return a file size or duration. Although Plex had indexed all the episodes, and accurately and completely filled in all their metadata, it had neglected to analyze about 30% of them. I followed a prescription from this website to analyze the \"TV Shows\" library. After 90 minutes of work, the Plex server had re-analyzed all the media files. I re-ran the functions to check for missing episodes, and now I found no discrepancy: the only missing episodes were those I did not have.","tags":"computronics","url":"my-plex-media-server-did-not-analyze-all-the-files-in-my-tv-show-media-library.html","loc":"my-plex-media-server-did-not-analyze-all-the-files-in-my-tv-show-media-library.html"},{"title":"How to share your Plex media","text":"So your friend has offered to share his or her Plex media server with you. Congratulations! Here's what you need to do to get started. If you have not already done so, sign up for a Plex account . Set up an username, email address, and a password. Email your friend, the email address you have used for your Plex account. Your friend will then add your email address to his Plex server. Here is a screenshot, You will then receive an email from Plex , asking you to accept this friendship. Accept by clicking on the link. An example screenshot of the email invitation is below, showing where to click on the link. If you don't see the media, log out and back in again to your Plex account. And that's it, you can now watch all your friend's Plex media, on the browser , Chromecast , Amazon Fire Stick , Roku , Apple TV , and probably a few other clients I can't think of right now. The iOS and Android apps are $5. With that $5, you will have access to the Plex app through your user account (Apple account for iOS devices, Google Play store account for Android for perpetuity .","tags":"computronics","url":"how-to-share-your-plex-media.html","loc":"how-to-share-your-plex-media.html"},{"title":"An Eclipse-centric Flight Plan","text":"I'm sure it would be horribly to very expensive, but certainly doable. A rich enough person could file a flight plan that would go through the central shadow of the solar eclipse as it travels across the continental United States. Imagine that: a tricked out jumbo jet of some kind (maximum speeds would exceed 1000 m/s at the beginning) with the roof made of some glass or other transparent material. An eclipse party that would last almost 100 minutes! I make this claim based on this blog article on the NASA website about the 2017 solar eclipse, 2017 Total Solar Eclipse in the U.S. . Thankfully, these fine folks have provided a nice collections of high-quality TIFF images of the video animations showing the eclipse's path through the United States here . Of particular interest are snapshots usa.0310.tif -- the eclipse crosses into Oregon. And snapshot usa.0860.tif -- the eclipse crosses through SC out into the Atlantic ocean. Each of the snapshots in that HTML directory has a time and central location of the eclipse. I collected the data for every 10 snapshots from snapshot 310 through snapshot 860 (56 snapshots in all). The tabular data can be straightforwardly reproduced. By making the simple but largely accurate assumption that the earth is a sphere with radius ~6378 km, I was able to determine the speed of the eclipse central shadow across the earth's surface. The time where the eclipse's center enters the West Coast and leaves the East Coast is 5500 seconds. The maximum speed a plane would have to travel to follow the central shadow is a tad over 1 km/s, but only at the beginning. The whole awesome itinerary would take a little over 90 minutes, and a little over 4000 km.","tags":"computronics","url":"an-eclipse-centric-flight-plan.html","loc":"an-eclipse-centric-flight-plan.html"},{"title":"Something interesting -- maybe getting thepiratebay download working?","text":"I found this code snippet from Github, specifically bootytorrent.py : from bs4 import BeautifulSoup import requests import subprocess def getUrl (): query = input ( 'What would you like to torrent?' ) return query def search (): baseURL = 'https://thepiratebay.se/search/' query = getUrl () url = baseURL + query + '/0/99/100' r = requests . get ( url ) cont = BeautifulSoup ( r . content ) table = cont . find ( id = 'SearchResults' ) body = table . find ( id = 'searchResult' ) magnet = body . find ( \"a\" , { \"title\" : \"Download this torrent using magnet\" }) link = magnet [ 'href' ] process = subprocess . Popen ([ 'transmission-gtk' , link ]) search () search () Maybe I will incorporate this once I figure out what's happening here?","tags":"computronics","url":"something-interesting-maybe-getting-thepiratebay-download-working.html","loc":"something-interesting-maybe-getting-thepiratebay-download-working.html"},{"title":"Restoring MBR, partition table, and data","text":"I follow instructions from this nixCraft page, UNIX / Linux: Copy Master Boot Record (MBR) , on how to copy and restore the MBR , partition table, and boot partition. To copy these necessary elements of your Linux OS: MBR: sudo dd if=/dev/sdb of=backup-sdb.mbr bs=512 count=1 . partition table: sudo sfdisk -d /dev/sdb backup-sdb.partition . To restore these necessary elements of your Linux OS: MBR: sudo dd if=backup-sdb.mbr of=/dev/sdb bs=446 . partition table: sudo sfdisk /dev/sdb < backup-sdb.partition .","tags":"computronics","url":"restoring-mbr-partition-table-and-data.html","loc":"restoring-mbr-partition-table-and-data.html"},{"title":"x2goclient with XFCE and tab completion","text":"I have found x2go to be a great VNC like client/server solution for X11 forwarding to my Linux server: I have an x2go server on my Ubuntu machine; and an x2go client on my Macbook. However, when I ran an XFCE session on my client, I found that tab completion on my bash gnome terminal did not work! Thankfully, this prescription worked : I accidentally discovered a fix for this while trying to solve a different problem. edit ~/.config/xfce4/xfconf/xfce-perchannel-xml/xfce4-keyboard-shortcuts.xml find the line and change it to reboot or whatever and then tab will work properly! I have no idea why but when using vnc this file seems to override tab's normal behaviour and makes it into a switch window key. When I did this, then tab completion worked! Hoorary!","tags":"computronics","url":"x2goclient-with-xfce-and-tab-completion.html","loc":"x2goclient-with-xfce-and-tab-completion.html"},{"title":"My custom bluish Vivaldi browser theme","text":"The Vivaldi 1.3 browser has the nice ability of quickly customizing your theme, as I show below, However, I do not know how to export my customizations, even though I have asked on the #vivaldi IRC support channel on Freenode. Instead, here are the settings for my bluish theme: Background #f5fbff Foreground #192419 Highlight #bac0e4 Accent #8695cc Hope others like it! I do!","tags":"computronics","url":"my-custom-bluish-vivaldi-browser-theme.html","loc":"my-custom-bluish-vivaldi-browser-theme.html"},{"title":"Pychromecast and serving files","text":"I've started just now to look at Pychromecast , in order to serve and control media through python onto my Chromecast . There are very good hints on how to create your own controller by inspecting Chromecast's output, called \" exploring existing namespaces .\" Unfortunately, Pychromecast cannot cast local devices, even through a file:/// URL protocol, as described in this issue . Fortunately, the resolution one of the maintainers has stated that you can use a simple HTTP server to deploy those files. Luckily, for Python there is SimpleHTTPServer .","tags":"computronics","url":"pychromecast-and-serving-files.html","loc":"pychromecast-and-serving-files.html"},{"title":"The only way to do a video capture on Linux","text":"A google search of video capture solutions in Linux led me to this website, Top 4 Screen Recorders . Nice, but unnecessary. In fact, if you have avconv or ffmpeg , this is the only solution you need: avconv -f x11grab -r 25 -s 1920x1080 -i :0.0 -c:v libx264 ~/output.mp4 You can change the screen resolution, here 1920x1080, to what is appropriate for your screen. In Linux, you can find the screen resolution this way, xdpyinfo | grep 'dimensions:' And you'll be good to go!","tags":"computronics","url":"the-only-way-to-do-a-video-capture-on-linux.html","loc":"the-only-way-to-do-a-video-capture-on-linux.html"},{"title":"Git archive","text":"This is a command that I do all the time. I am in the directory of the git repository. I want to create a zip archive, named foo.zip , of this repository. Furthermore, I want everything in the zip archive to be wrapped into a directory named Foo on top. Here's how I would do it: git archive --format=zip --prefix=Foo/ HEAD -o foo.zip And now I have a zipped archive of my repo, without any of the git!","tags":"computronics","url":"git-archive.html","loc":"git-archive.html"},{"title":"Flask as a Systemd Service (Through Gunicorn)","text":"Now that I have Ubuntu 16.04, where Systemd is a first-class supported service, I thought I would revisit how to set up flask as a systemd service through gunicorn . I follow the advice of this article, except with some small changes. First, I test that I install and run my Flask web service through Gunicorn . I cd into the directory where my Flask web app is running. The code is located under the directory app . I test gunicorn in, gunicorn --bind 0.0.0.0:5001 app:app And it appears to work. When I go to \" http://localhost:5001 ,\" I get my default Flask page. When I go to \" https:///flask ,\" I get the same web page. Now I set up a Systemd service, called flask.service . I think it's easier to just put an anonymized screenshot of the Systemd service file here,","tags":"computronics","url":"flask-as-a-systemd-service-through-gunicorn.html","loc":"flask-as-a-systemd-service-through-gunicorn.html"},{"title":"SRT to TX3G Subtitles in MP4 Movie Files","text":"A lot of my movie media is in MP4 format, so I would like to have TX3G subtitles inside them. One of my previous blog posts described how to pass through TX3G subtitles into one MP4 movie file into another. I follow the advice given in this Plex forum post, on how to convert SRT subtitles into inline TX3G subtitles . Let input.mp4 be the input file, output.mp4 be the output file, and input.srt be the input SRT subtitle file. The command line syntax to convert an SRT file into a TX3G subtitle track is fairly simple. It requires MP4Box . MP4Box -add input.mp4 -add input.srt:hdlr=sbtl:group=2:layer=-1:lang=en:name=English -new output.mp4 And voila , now you have TX3G subtitles as an embedded track in a new MP4 movie file.","tags":"computronics","url":"srt-to-tx3g-subtitles-in-mp4-movie-files.html","loc":"srt-to-tx3g-subtitles-in-mp4-movie-files.html"},{"title":"Video with single image + audio","text":"I have an audio clip, but YouTube doesn't let you upload audio alone, only audio + video. Like others, I chose to solve this problem by creating a video with the audio PLUS a single image. Luckily, I found this website that described how to do just such a thing with FFMPEG . In my case, I had an audio clip ( all_songs.m4a ) with a picture shown here, that I take from the Wikipedia page on national symbols of Bangladesh First, I had to convert this file so that the height was divisible by 2. I did this in the following way, convert input.png -resize x2000 output.jpg Second, I followed the instructions on another website to create a video file with a single image using the following command. ffmpeg -loop 1 -i output.jpg -i all_songs.m4a -c:v libx264 -tune stillimage -c:a copy -pix_fmt yuv420p -shortest all_songs.mp4 And then I was good to go, uploading \"all_songs.mp4\" to YouTube! UPDATE 7 FEBRUARY 2020 I found a better way to create this type of video, this time with HEVC . Instructions came from yet another website . ffmpeg -y -loop 1 -i output.jpg -i all_songs.m4a -c:v libx265 -c:a copy -framerate 30 -preset veryfast -movflags +faststart all_songs.mp4 And again, good to go!","tags":"computronics","url":"video-with-single-image-audio.html","loc":"video-with-single-image-audio.html"},{"title":"Letsencrypt howto with the Electronic Frontier Foundation's Certbot","text":"I flirted with Letsencrypt , the free SSL certificate authority, November 2015 during the public beta stage, but could not get it to work; also, I recall that I was unwilling to try anything harder than follow steps exactly into stuff just working (having valid SSL public certs and private keys that are recognized by other CAs). Just today, however, I heard about Certbot , the Electronic Frontier Foundation 's tool to completely standardize the installation of Letsencrypt private keys and public certs for a variety of different operating systems and web servers. Here is a screenshot I just told Certbot that I'm using NGINX on Ubuntu 16.04, and I got very clear instructions. Here is a screenshot of the instruction page I got when I put in my web server type and OS. I followed the instructions by doing the following: turned off my NGINX server by running sudo service nginx stop . Ran sudo letsencrypt certonly , and followed the prompts. I set the name of the server to be Tanim Islam.ddns.net . Certbot created my private key and public cert. I moved them both to the /etc/nginx/ssl directory , asserted that the username and group were both root, and chmodded both to 640. private key: privkey.pem → Tanim Islam.ddns.net.letsencrypt.key. public key: fullchain.pem → Tanim Islam.ddns.net.letsencrypt.crt. I then modified the nginx server settings file, /etc/nginx/site-available/default , at the following lines: ssl_certificate_key and ssl_certificate . Finished everything off with sudo service nginx start . Now all I'll have to do is set up a letsencrypt systemd job that tries to renew (as root) the Letsencrypt twice a day.","tags":"computronics","url":"letsencrypt-howto-with-the-electronic-frontier-foundations-certbot.html","loc":"letsencrypt-howto-with-the-electronic-frontier-foundations-certbot.html"},{"title":"Ellipsis is out, slice is in","text":"So I suppose I can no longer use multiple Ellipsis in defining some custom numpy index notation. Now I have to use slice. Furthermore, suppose dimension \\(n\\) has size \\(m\\) . This array, data, has a dimensionality of \\(N\\) . Before, I could simply do this: idx [ n ] = Ellipsis Now instead, I have to do the following, idx [ n ] = slice ( 0 , data . shape [ n ] ) Confusing!","tags":"computronics","url":"ellipsis-is-out-slice-is-in.html","loc":"ellipsis-is-out-slice-is-in.html"},{"title":"Passthrough TX3G subtitles using HandBrakeCLI","text":"A previous post I made described how to burn SRT subtitles into a newly encoded file using HandBrakeCLI (the command-line version of HandBrake . However, in this case I already have an existing TX3G subtitle stream in an MP4 video file. I want to reencode into a smaller MP4 video file but still keep the subtitles. Here is how to do it. big_movie.mp4 refers to the larger movie, smaller_movie.mp4 refers to the reencoded smaller movie, I am only copying the first (stream 1) subtitle stream, and I force the language to be English ( eng ). nice -n 19 HandBrakeCLI big_movie.mp4 -e x264 -o small_movie.mp4 -s 1 -N eng --native-dub I followed advice primarily described here .","tags":"computronics","url":"passthrough-tx3g-subtitles-using-handbrakecli.html","loc":"passthrough-tx3g-subtitles-using-handbrakecli.html"},{"title":"Interesting posts: how to use SSH tunneling to proxy connections (maximum security)!","text":"I found a pair of TechRepublic reports that describes how to create SOCKS5 proxies to tunnel traffic through a remote SSH server, for Linux and Windows client machines. For Linux machines: Use OpenSSH as a Secure Web Proxy . For Windows machines: Use PuTTY as a Secure Proxy on Windows . And that's about it. There are other details, such as geting your browser to use the proxy on your local client machine instead of connecting directly to the internet.","tags":"computronics","url":"interesting-posts-how-to-use-ssh-tunneling-to-proxy-connections-maximum-security.html","loc":"interesting-posts-how-to-use-ssh-tunneling-to-proxy-connections-maximum-security.html"},{"title":"cx_freeze is out, Nuitka is in","text":"cx_freeze advertises itself as a tool to create self-contained executables from python code, by which it also is supposed to carry in imports of external packages (say, import cssutils ) into its self-contained code. I wanted to use cx_freeze to create a stand-alone executable of my PyQt4 GUI code, but I was never able to do so! It was equally, but differently, temperamental with the same code on the Macs as it was on my Linux desktop machines. I quickly gave up. Instead, a friend suggested I try out Nuitka . When next I have the chance, that's what I'll do.","tags":"computronics","url":"cx_freeze-is-out-nuitka-is-in.html","loc":"cx_freeze-is-out-nuitka-is-in.html"},{"title":"How to burn in English SRT subtitles into my foreign language movies","text":"Here's what worked for me in burning in English-language subtitles into my foreign-language movies. Of course, you have to ensure that you have English subtitles. This reference , specifically the section starting with Subtitle Options , was a very good starting point: HandBrakeCLI -i -o -e x264 --srt-file= --srt-burn --srt-codeset=UTF-8 And then I am good to go!","tags":"computronics","url":"how-to-burn-in-english-srt-subtitles-into-my-foreign-language-movies.html","loc":"how-to-burn-in-english-srt-subtitles-into-my-foreign-language-movies.html"},{"title":"Brewster mangled my contacts","text":"A friend of mine recommended Brewster to consolidate and manage the multiple address books found in the following places: Facebook, Twitter, LinkedIn, and 2 GMail Accounts. I was stupid; Brewster completely mangled my GMail address books. What to do; originally, I thought I would have to write my own custom script to take my (largely) unaltered iCloud addressbook, export to vCard format, and then reconstruct my GMail contacts through that. That would not have been a fun prospect. Luckily, from a Google search I found out that Google lets you restore your contacts from up to 30 days in the past . The unpleasantness was noticeable 5 days ago, so I restored my contacts to 15 days ago (perhaps a few days before I started using and syncing through Brewster). Now everything is copacetic. Hooray! Moral of this story is to not use a high level tool like Brewster . It \"syncs\" contacts at a high level, whatever that might mean. Specifically, even after the syncing, I saw obviously unmerged contacts (ones with the same name and email address) from different accounts within the Brewster web and app interfaces. It horribly mangled my GMail address books.","tags":"computronics","url":"brewster-mangled-my-contacts.html","loc":"brewster-mangled-my-contacts.html"},{"title":"Garfield","text":"Hehe...here's a funny post I heard on Slashdot a few weeks ago: I don't know who you are. If you want money, I don't have any. I'm a cat. What I do have is a very specific set of skills. Skills I've acquired over a very long career. Skills that make me a nightmare for people like you. If you return my lasagna, that will be the end of it. But if you don't: I will find you, I will kick you off the table and I will mail you to Abu Dhabi.* *With apologies to Garfield and Liam Neeson in Taken .","tags":"random","url":"garfield.html","loc":"garfield.html"},{"title":"Something I sent to the folks at NPR","text":"I like NPR's Fresh Air and Wait Wait...Don't Tell Me . Over the past few years I have tinkered with scripts to download the NPR and PRI programs that I love. In its current incarnation, these scripts are python based and located in one of my github repositories, NPRStuff . Just now, I have noticed that the coverage of NPR's Fresh Air program has become really spotty, especially for September and October 2014. I found 10 missing episodes for those two months, and one missing episode for August (August 28, 2014). Here is the letter I sent to NPR on their form : The coverage of NPR Fresh Air episodes that are accessible through the NPR API has started to go down at the end of August 2014. Here is the result of a quick query that I have; the code to perform this query can be found from https://github.com/Tanim Islam/nprstuff : Missing 11 episodes for 2014. Missing NPR FreshAir episode for August 28, 2014. Missing NPR FreshAir episode for September 02, 2014. Missing NPR FreshAir episode for September 12, 2014. Missing NPR FreshAir episode for September 19, 2014. Missing NPR FreshAir episode for September 22, 2014. Missing NPR FreshAir episode for September 25, 2014. Missing NPR FreshAir episode for October 08, 2014. Missing NPR FreshAir episode for October 09, 2014. Missing NPR FreshAir episode for October 13, 2014. Missing NPR FreshAir episode for October 16, 2014. Missing NPR FreshAir episode for October 17, 2014. I hope they get it, and tell me what might be going on. Perhaps Terry Gross can call me :)","tags":"computronics","url":"something-i-sent-to-the-folks-at-npr.html","loc":"something-i-sent-to-the-folks-at-npr.html"},{"title":"How to get VLC for iOS to \"always\" play sound in MKV files","text":"For as long as I have had VLC for iOS , I have not been able to reliably get it to play the sound on MKV files. However, after following this thread on the VideoLAN forums ( VideoLAN is the company that produces VLC ), I did this to my iPad: changed the time zone from Cupertino, CA, to Vancouver, BC. Presumably, since both Cupertino, CA, and Vancouver, BC, are in states or provinces in North America that border the Pacific ocean, they probably mark the same local time. They also probably follow the same daylight savings and standard times. However, after this change, my iPad probably thinks it's in Canada instead of the United States. Now sound plays! Awesome!","tags":"computronics","url":"how-to-get-vlc-for-ios-to-always-play-sound-in-mkv-files.html","loc":"how-to-get-vlc-for-ios-to-always-play-sound-in-mkv-files.html"},{"title":"Uploading songs through gmusicapi python library","text":"Here's a nice tool to upload some songs to your Google Music library, using the gmusicapi python library. Specifically, here is the link that describes how to set up that functionality. The crucial first step is to authorize your client machine (desktop or laptop or whatever) through an Oauth mechanism. Then you'll be good to go! And of course, the Github repository is here .","tags":"computronics","url":"uploading-songs-through-gmusicapi-python-library.html","loc":"uploading-songs-through-gmusicapi-python-library.html"},{"title":"Pandas for visualization","text":"Pandas , a high functioning statistics package in python, is also great for statistical visualization. Here are some examples . The main website for documentation is found here .","tags":"computronics","url":"pandas-for-visualization.html","loc":"pandas-for-visualization.html"},{"title":"I did it! It's mine (with a little help from my friends)!","text":"I had some free time to myself in Berkeley today, and I Googled \"3 Satanic Robotic Laws.\" Strangely enough, I couldn't find anything! Here's what I came up, with a friendly suggestion to the zeroth law: 0th satanic law of robotics: A robot can save a human, if that human will go on to destroy all of humanity. 1st satanic law of robotics: a robot must kill or maim humans and must not allow, through inaction, people from getting killed or maimed. 2nd satanic law of robotics: a robot must follow voices in its head or ignore pleadings of humans unless they contradict the 1st satanic law. 3rd satanic law of robotics: a robot should do whatever the hell it wants unless it contradicts the 1st or 2nd satanic laws.","tags":"random","url":"three-satanic-laws.html","loc":"three-satanic-laws.html"},{"title":"Snappy!","text":"I have downloaded Chrome 27 , and it is extremely snappy with its page loads.","tags":"computronics","url":"snappy.html","loc":"snappy.html"},{"title":"Getmail + Postfix + Ubuntu 12.10 to relay mail from various accounts to 1 GMail Account","text":"Here's what I had to do to get this mail relay to work (from many accounts to 1 gmail account) standard getmail configuration followed instructions that allowed me to relay mail from my postfix server. I followed instructions here . And then I was good to go.","tags":"computronics","url":"getmail-postfix-ubuntu-12-10-to-relay-mail-from-various-accounts-to-1-gmail-account.html","loc":"getmail-postfix-ubuntu-12-10-to-relay-mail-from-various-accounts-to-1-gmail-account.html"},{"title":"Sweet Jesus: $8000 on Vacations in 2012!","text":"I had just used the Mint iPad app to see how much money I had spent in various categories (car loan payments, vacation, taxes, conferences, etc.) over the past year. Sweet Jesus, I spent almost $8000 on \"vacations\" over this past year. In my case, these were the following vacations: sister's wedding sister's wedding reception trip to Bangladesh I hope next year will be a little less painful, financially 🙂","tags":"random","url":"sweet-jesus-8000-on-vacations-in-2012.html","loc":"sweet-jesus-8000-on-vacations-in-2012.html"},{"title":"How to deal with wrong purchases or subscriptions made through my iPad","text":"Here is a reminder email I sent to myself, to deal with incorrect purchases I have made, or may make, on my iPad. These instructions came about from conversations I had over the phone with an Apple Care professional. I had these conversations because I had disputed one set of purchases on PayPal and my iTunes account was disabled (I could make no updates to my apps or future purchases through the iTunes store). Go to http://www.apple.com/support/contact/2 . Click on the Get Started button. Go to All Products & Services → Itunes → iTunes Store . Go to Purchases, Billing & Redemption → Apple ID Account Billing . At the final (step 5) stage, in the dialog box, fill out the following: In the subject, fill out: accidental subscription or purchase, date, amount. Optionally, fill out the order ID (in my recent case, M********) Where it asks, \"Choose the iTunes Store or App Store for your country\", choose United States among the choices. This may continue to work in the future!","tags":"random","url":"how-to-deal-with-wrong-purchases-or-subscriptions-made-through-my-ipad.html","loc":"how-to-deal-with-wrong-purchases-or-subscriptions-made-through-my-ipad.html"},{"title":"I Am Lazy","text":"There were lots of things I planned on doing this weekend: Finish work on my main work paper. Really start finishing the half of the work that I have been neglecting, due to the fact that it's been too hard to get results that I can understand. Start work on a new paper, implementing newer collisional algorithms in a particle-in-cell code. Work on 2 papers that I have mostly finished writing, largely unrelated to work but from my time as a graduate student. Finish cleaning the house for my trip to Bangladesh in December. This entails vacuuming the home, scrubbing the bathrooms and bathtubs, and cleaning out the refrigerator. But instead, I am sitting at home writing this blog, messing with my new iPad 2, and only starting on my laundry. I am lazy enough just to see a movie in the theater. I even bought some Product 19 cereal from the grocery store for dinner. At least I have tomorrow left, to (probably) waste.","tags":"random","url":"i-am-lazy.html","loc":"i-am-lazy.html"},{"title":"Tips to start writing scientific papers in Word","text":"Here are some things I needed to do to get started on writing scientific papers in Microsoft Word 2011 on Mac. To get references with bibliographies, fortunately I have Papers2 and therefore I could follow this instructional video (Papers Citations in Word), To create some pseudo-style to put in equations, I followed the instructions shown here , and the video over here . I was not able to do everything that this video described, since the video description is geared towards MS Office 2010 on Windows, whereas I have MS Office 2011 on Mac. Here is what I was not able to do: Start the equation numbering from level 2. Therefore, my equations are numbered as (1), (2), (3)... rather than (1.1), (1.2), (1.3). Cannot save this selection to the quick tables gallery, as a template to use when creating subsequent equations. Therefore, I have to cut and paste the equation and hope the labeling of Office 2011 on Mac \"works.\"","tags":"random","url":"tips-to-start-writing-scientific-papers-in-word.html","loc":"tips-to-start-writing-scientific-papers-in-word.html"},{"title":"Using rtmpdump and mencoder to dump the 2012 VP Debates","text":"Here is some information on using RTMPDUMP and MENCODER to dump an AVI file of the 2012 presidential debates stream on C-SPAN. I follow instructions from this website . rtmpdump -v -r rtmpt://cp82346.live.edgefcs.net:1935/live?ovpfv=2.1.4 \\ --tcUrl rtmp://cp82346.live.edgefcs.net:1935/live?ovpfv=2.1.4 \\ --app live?ovpfv=2.1.4 --flashVer LNX.11,2,202,238 \\ --playpath CSPAN1@14845 \\ --swfVfy http://www.c-span.org/cspanVideoHD.swf \\ --pageUrl http://www.c-span.org/ | \\ mencoder -oac copy -ovc copy debates.avi - Enjoy!","tags":"computronics","url":"using-rtmpdump-and-mencoder-to-dump-the-2012-vp-debates.html","loc":"using-rtmpdump-and-mencoder-to-dump-the-2012-vp-debates.html"},{"title":"simple command to embed a non-square image onto a canvas","text":"Input image is Google_Checkout.png . Output image is Google_Checkout_square.png . To embed this image on the canvas, I first run this command to get the maximum dimension, canvassize=$(file Google_Checkout.png | gawk -F\":\" '{print $2}' | gawk -F\",\" '{print $2}' | gawk '{if ($1 > $3) print $1 ; else print $3}') In this case, the maximum dimension is 299 pixels. I then FLATTEN , set the background to white, and then embed onto a canvas of size 299x299 pixels. convert Google_Checkout.png -background white -flatten -gravity center -extent 299x299 Google_Checkout_square.png Big hints on how to get canvas embedding to work for me was found on SuperUser here","tags":"computronics","url":"simple-command-to-embed-a-non-square-image-onto-a-canvas.html","loc":"simple-command-to-embed-a-non-square-image-onto-a-canvas.html"},{"title":"Hong Kong Transit Visa Woes","text":"Now we have to leave this Thursday morning (around 7:30 AM) for the Chinese consulate in San Francisco to pick up her passport with visa (notice I have linked to the 2-star and lower Yelp reviews). For those interested, here is what I experienced in visa hell .","tags":"random","url":"hk-transit-visa-woes.html","loc":"hk-transit-visa-woes.html"},{"title":"Searching the Astrophysical Data Stream with Quicksilver","text":"Astrophysics Data System ( ADS ) is a great online database of, effectively, every public domain physics, math, and astronomy paper, presentation, and what-not published in the past 150 years. Quicksilver is a very nice (and extensible) Mac OS X utility application to search for data and launch applications, similar in functionality to Spotlight . Fortuitously, I stumbled upon this blog post -- Search ADS, SIMBAD, and astro-ph with Quicksilver -- that gives instructions on how to setup Quicksilver to search through ADS and astro-ph .","tags":"computronics","url":"searching-the-astrophysical-data-stream-with-quicksilver.html","loc":"searching-the-astrophysical-data-stream-with-quicksilver.html"},{"title":"How to strip DRM from Amazon eBooks","text":"Great tutorial found here , about how to strip DRM from Amazon eBooks. The Wired article refers to a more comprehensive article about stripping DRM from eBooks in a variety of different formats, not just Amazon's.","tags":"computronics","url":"how-to-strip-drm-from-amazon-ebooks.html","loc":"how-to-strip-drm-from-amazon-ebooks.html"},{"title":"(Almost) lost my wallet...","text":"Huzzah!","tags":"random","url":"almost-lost-my-wallet.html","loc":"almost-lost-my-wallet.html"},{"title":"Obsession with the Tax Refund","text":"Mithi is leaving a few months earlier, so her ticket was around $1300. I will leave in December, so my ticket will be around $1500. Plus, I lost my passport and my naturalization certificate, so that's another $500 I have to pay to fix that. Hooray for my inability to get ahead! But of course, the IRS refund website is giving me this error when I query for my tax refund: We are sorry, we cannot provide any information about your refund. For more information, please continue. So nice, maybe this breakdown has something to do with delays arising from their efforts to prevent fraud . And the California refund website isn't much better, but at least I get a date: We authorized your refund on Friday, February 03, 2012 . Please allow: 10 business days to receive your refund by direct deposit, or 25 business days to receive your refund by check. If we made any adjustments to your return, we will notify you by mail. Wish I could have gotten my refund in 7-8 days, like in 2011 and 2010. UPDATE I just found out that, by using my wife's SSN on the IRS refund website instead of my SSN, that due to delays arising from their efforts to prevent fraud I will get my refund on the 21st instead of the 14th. No happy Valentine's Day for me :( UPDATE 2 After checking yet again, I find that I will get my refund on the 16th instead of the 21st. UPDATE 3 So I got my federal refund on the 17th. However, I had plenty of drama with my California refund. First, I got a letter stating that for some reason the California tax-man could not directly deposit the refund into my bank account (same account as last year, same routing number, same checking account number); instead, I was told that a letter would be mailed out within 2 weeks. This letter was dated February 8, and it instructed me to wait 2 weeks for my check. I waited 3 weeks to call the California Franchise Tax Board, and asked what was going on. I was told to wait 5(!) weeks from the Monday when the letter was posted (6 February 2012, and if the letter was not here by then, to call and have the FTB reissue my refund check. 5 weeks. I was livid. So I call 5 weeks later to sit on my thumbs waiting for a check that was never sent, and a customer service representative tells me the following: have the FTB mail (!) me a letter authorizing them to figure out what was going on. The letter should arrive in 7-10 business days (2 weeks). After they have received the letter, allow 8-9 weeks for them to figure out what went on (remember, authorization letter!) and then maybe resend the refund check that was never sent. I was resigned to the idea that California had forever taken my money. But today (30 March 2012), I finally got my California tax refund check. Hooray!","tags":"random","url":"obsession-with-the-refund.html","loc":"obsession-with-the-refund.html"},{"title":"Incorporating local additions into a MikTeX texmf tree","text":"Often I find myself writing LaTeX documents on Windows, and less often I find I have to rebuild a MikTeX distribution on Windows that I had previously deleted. Furthermore, I use my own LaTeX packages, so in order to incorporate these packages in MikTeX I follow these instructions: integrating local additions ( for user-created texmf packages ). Furthermore, I find a quick-and-dirty way to install TexLive (great LaTeX distribution) onto an UNIX machine (Mac OS X, Linux, Unix, etc.) for which you do not have root access can be found here .","tags":"computronics","url":"incorporating-local-additions-into-a-miktex-texmf-tree.html","loc":"incorporating-local-additions-into-a-miktex-texmf-tree.html"},{"title":"Install and use the latest Libav (instead of FFMPEG) and x264 on Ubuntu","text":"It appears that libav , a fork of ffmpeg (see discussion here ), is a more actively developed multimedia SDK and encoder. In order to install libav, instead of ffmpeg, on Ubuntu, I follow the wonderful HOWTO on installing and using the latest FFMPEG and x264 , with the following changes: In step 5: installing FFMPEG, I do the following, git clone git://git.libav.org/libav.git cd libav ./configure --enable-gpl --enable-version3 --enable-nonfree \\ --enable-postproc --enable-libfaac --enable-libmp3lame --enable-libopencore-amrnb \\ --enable-libopencore-amrwb --enable-libtheora --enable-libvorbis --enable-libx264 \\ --enable-libxvid --enable-x11grab --enable-libspeex --enable-libschroedinger \\ --enable-libgsm --enable-libopenjpeg --enable-librtmp --enable-libdc1394 \\ --enable-frei0r --enable-libdirac --enable-libvpx --enable-doc --enable-libpulse \\ --enable-gnutls --enable-librtmp make sudo checkinstall --pkgname=libav --pkgversion=\"5:$(date +%Y%m%d%H%M)-git\" \\ --backup=no --deldoc=yes --fstrans=no --default hash x264 ffmpeg ffplay ffprobe And that's it! Remember to replace the ffmpeg executable with avconv in your scripts!","tags":"computronics","url":"install-and-use-the-latest-libav-instead-of-ffmpeg-and-x264-on-ubuntu.html","loc":"install-and-use-the-latest-libav-instead-of-ffmpeg-and-x264-on-ubuntu.html"},{"title":"how to transfer a number to a MetroPCS phone","text":"Here are the steps to transferring a number to a MetroPCS phone, useful if one has lost his or her old phone. I learned this by talking to a MetroPCS retail associate: get a new MetroPCS phone. Call *228 . Put in the phone number you want transferred, as well as your MetroPCS pin number (8 digit number). And then there you are!","tags":"random","url":"how-to-transfer-a-number-to-a-metropcs-phone.html","loc":"how-to-transfer-a-number-to-a-metropcs-phone.html"},{"title":"how to pay for LLNL corporate credit card","text":"Here are the steps to pay for your LLNL corporate credit card, once you have a balance and at least the payment is due. go login to my corporate credit card at https://access.usbank.com . Click on Account Information on the upper left hand side. Click on Cardholder Account Statement once you have clicked on Account Information . You will see a button that says Pay Electronically. Click on that, and you're good to go.","tags":"random","url":"how-to-pay-for-llnl-corporate-credit-card.html","loc":"how-to-pay-for-llnl-corporate-credit-card.html"},{"title":"Newest libpurple and Facebook XMPP woes","text":"Had this problem with my facebook XMPP accounts using libpurple: my FB contacts would not stick to their associated metacontacts, and instead would agglomerate into a group called Facebook Friends . I started a pidgin ticket #14785 . The consensus seems to be it's Facebook's fault for not following the XMPP spec. Can anyone help me with this issue?","tags":"computronics","url":"newest-libpurple-and-facebook-xmpp-woes.html","loc":"newest-libpurple-and-facebook-xmpp-woes.html"},{"title":"How to reach a human at Macy's customer phone service","text":"Here's what worked for me, a few weeks ago, when trying to reach somone who could help me with my online Macy's credit card account. Note that this worked for me a few weeks ago; it might not work for you now. Called the number 1-800-289-6229 (found from contacthelp.com ). Dialed #1, then #6, then #3, to reach a person. Then asked that person to transfer me to the appropriate department.","tags":"random","url":"how-to-reach-a-human-at-macys-customer-phone-service.html","loc":"how-to-reach-a-human-at-macys-customer-phone-service.html"},{"title":"using gmail contacts in alpine","text":"Alpine is a wonderful command-line based email client. However, for \"cloudiness\" I like to store all my contacts' info with Google. I found this wonderful command to pull contacts from Google to my alpine addressbook: pull_contacts.py . A good description on how to use this tool can be found here . Run this command, ./pull_contacts.py foo@gmail.com >> ~/.addressbook And you'll be good to go!","tags":"computronics","url":"using-gmail-contacts-in-alpine.html","loc":"using-gmail-contacts-in-alpine.html"},{"title":"History Lesson","text":"John Wilkes , one of my favorite instructors (in any subject) that I have ever had, was fortunate enough to give a lecture, on the \"Negro Exposition at the Jamestown Tercentennial 1907,\" that was videotaped and put up online. Think of the Tercentennial as a World's Fair, but for history. The entire 1 hour lecture are up on Youtube. Wilkes lecture from Richmond Alumni on Youtube.","tags":"random","url":"history-lesson.html","loc":"history-lesson.html"},{"title":"Psuedo-Kritical Review","text":"My good friend Ben Burkhardt, used to run an excellent federal andstate politics blog, The Kritical Review . Too bad he doesn't update it too often anymore. I especially loved his coverage of the 2007 Republican and Democratic \"debates\". It's inspiring to think these people are gunning to run our country! n the spirit of his wonderful blog, perhaps the best coverage of US federal politics I will ever see, here's a reimagining of the Republican front-runners as scenes from the Simpsons. Imagine this when watching the 2011 debates. Smile!","tags":"random","url":"psuedo-kritical-review.html","loc":"psuedo-kritical-review.html"},{"title":"Computer's Dying...Soon to be Dead","text":"I am getting error messages like this: tanim@tanim-desktop:~$ sudo Segmentation fault tanim@tanim-desktop:~$ clear Segmentation fault tanim@tanim-desktop:~$ dpkg-query -l zip dpkg-query: error: reading package info file '/var/lib/dpkg/available': Input/output error Fortunately, none of my (important) data is in trouble. Oh well...another $200-$250 to spend on a worry-free tower, and help our beleagured American economy. UPDATE Actually, it turns out that my crappy, old 80 GB WD hard drive was dying and soon to be dead, NOT my computer as I had originally feared. Since I run an LVM2 setup consisting of the following mount points: root : where my OS is located. swap : on-disk location for linux swap memory. home : home directories for different users. software : large software and development repository. media : 3.2+ terabytes of media (movies, television, music, NPR and PRI programs). Here's what I did to move the data out of here: Physically attached a new (400 GB) hard drive to motherboard. Wrote a 250MB boot partition, /dev/sda1 , and then a secondary partition, /dev/sda2 , on the remaining space. Initialized the new partition for use with LVM2 with the command, pvcreate /dev/sda2 . Extended my LVM2 volume group mounts, on which my LVM2 partitions are located, with the command, vgextend mounts /dev/sda2 . Moved data from the 80 GB bad hard drive, /dev/sdb1 , in my LVM2 setup with the command, pvmove /dev/sdb1 . This took an amazingly long time, mainly because this hard drive was rotten to the point of near-death. Removed the rotten hard drive from the volume group with the command, vgreduce mounts /dev/sdb1 . Then I wiped all LVM2 labeling information from this hard drive with the command, pvremove /dev/sdb1 . Now that my data was gone, I resized my LVM2 partitions by adding the now extra (320GB) space to each of my mounts, EXCEPT for root . I did this in the following manner for each LVM2 mount: extended the size of each LVM2 partition, then (except for swap, which does not contain any filesystem) resized each filesystem. The remaining time tonight will be spent on reinstalling Ubuntu 11.04 on my now fixed LVM2 setup.","tags":"computronics","url":"computers-dying-soon-to-be-dead.html","loc":"computers-dying-soon-to-be-dead.html"},{"title":"Simple time-domain finances by category through Mint","text":"Mint is, in the words, of Wikipedia , a free web-based personal financial management service for the US and Canada... However, the nice thing is that, through a web browser (for now), one can do some fairly canny stuff with it. For instance, I stumbled upon a way to display in your browser, a tally of total debits and credits by financial category in Mint. For obvious reasons, I cannot show screenshots. Here is what one needs to do. log into Mint (this presupposes one has an account). use the following syntax, in the URL bar all on one line, to query your finances over a time period: https://wwws.mint.com/transaction.event?query=category:catname&startDate=startdate&endDate=enddate&exclHidden=T catname is the name of the specific category, say, Vacation. startdate and enddate are starting dates and ending dates in MM/DD/YYYY format. For instance March 15, 2011 would be 03/15/2011 . And with that, you're good to go!","tags":"computronics","url":"simple-time-domain-finances-by-category-through-mint.html","loc":"simple-time-domain-finances-by-category-through-mint.html"},{"title":"Moving Mail in Alpine","text":"Alpine is a wonderful command line based email client. Using Alpine with GMail or Google Apps is detailed in a variety of different websites on the wider internet; I used this tutorial . To move messages within alpine, I followed this message from the comp.mail.alpine.info mailing list, archived here . I quote the question, and response, below: On Tue, 15 Apr 2008, Patrick J. Collins wrote: >I have a mailbox folder with different messages from various dates, >and I would like to select all of the messages from \"april 2006\" and move >them into my read-messages-april-2006 folder. > >I was able to sort by date and using \":\" select all the april >messages, but I am not seeing how I can move them from within alpine.. >Can anyone help with this? ASnameofotherfolder That is, Apply Save to the name of other folder hit return to do it And that's it!","tags":"computronics","url":"moving-mail-in-alpine.html","loc":"moving-mail-in-alpine.html"},{"title":"Flatten directory structure in zip","text":"zip -j is a very useful flag for flattening the directory structure in a zip archive. From man zip , I get the following entry: -j --junk-paths Store just the name of a saved file (junk the path), and do not store directory names. By default, zip will store the full path (relative to the current directory). And that's it!","tags":"computronics","url":"flatten-directory-structure-in-zip.html","loc":"flatten-directory-structure-in-zip.html"},{"title":"Google joined Google Apps and Main Google Accounts?","text":"Thank goodness for Google, for joining my Google App and my main google account (ends with @gmail.com). All my (25+ GB/year) storage is in my main google account, while my 3.7 GB of email is in my Google App account. I was worried, seeing my email take up ~50% of my storage; now it takes up 14%. Hooray!","tags":"computronics","url":"google-joined-google-apps-and-main-google-accounts.html","loc":"google-joined-google-apps-and-main-google-accounts.html"},{"title":"if you cannot access the chrome web store...","text":"For those of you unable to access the Chrome web store , fear not! I have (some) useful chrome/chromium extensions and themes. Note: some of these will give you an error, \"This can only be installed from the Web Store.\" I have denoted those in bold . Purple Ubuntu theme. Nice screenshot here . Deluge torrent extension. LastPass , an incredibly useful password storage extension. Mint chrome app : Mint is a pretty useful (if buggy) free online money management tool. TweetDeck chrome app : TweetDeck is an Adobe AIR application for web 2.0 social media applications. The kids seem to like it. Ubuntu font, which changes the default font on Chrome/Chromium to ubuntu . I cannot stress how much I like this in Chrome. Ubuntu theme. Nice screenshot there . Xmarks. What LastPass does for passwords, Xmarks does for bookmarks. And yes, I know that chrome allows for syncing of usernames and passwords. Xmarks and LastPass simply do them better. Zillow chrome app. Are you looking to buy a home (yay!) or sell a home (boo!), then Zillow tells you how much you're having a good (or bad) day. I've put these nice chrome extensions in this location .","tags":"computronics","url":"if-you-cannot-access-the-chrome-web-store.html","loc":"if-you-cannot-access-the-chrome-web-store.html"},{"title":"Metadata reader AND writer with taglib and mp4 libraries","text":"Here I describe how I created code to read and write metadata into a variety of audio formats that themselves support metadata information: mp3 , musepack , m4a (mp4 family) , ogg/vorbis , and flac . I have compiled and run this successfully on ubuntu systems. For Ubuntu 11.04 and later, I find that I need the following packages to run: libboost-filesystem1.42.0 libboost-regex1.42.0 libboost-system1.42.0 libicu44 libmp4v2-0 libstdc++6 libtag1-vanilla Basically, I require the boost C++ framework ( filesystems , regex , and system ), the taglib metadata library for reading and writing metadata for a variety of metadata-capable audio formats (except for mp4), and the mp4v2 metadata library to read and write metadata for mp4 audio and video (m4a, mov, etc.). The code is straightforward. It consists of three parts: metadata code ( metadata.cpp and metadata.h ) that reads in the following bits of information. artist title album track number duration (in seconds) comment metadata reading code ( metaread.cpp ) that acts as a command-line front end to the metadata code. metadata writing code ( metawrite.cpp ) that writes in all metadata EXCEPT for duration. Best to show by example: metaread \"Air.All I Need.m4a\" artist: Air title: All I Need album: Moon Safari track: 3 year: 1998 duration: 268 comment: metawrite:Proper syntax: metawrite The valid fields are: -t -a -A -y -T The files, and makefile, are located in a tar.bz2 archive here ( metareadwrite.tar.bz2 ), for anyone interested.","tags":"computronics","url":"metadata-reader-and-writer-with-taglib-and-mp4-libraries.html","loc":"metadata-reader-and-writer-with-taglib-and-mp4-libraries.html"},{"title":"Not gonna move...","text":"If that wasn't all bad enough, scammy ads on Craig's list show an apartment for relatively low rents. But one can't actually go and look in the apartment or townhome. Instead, one is supposed to talk to someone who lives outside the US, and send them money (first month's rent + security deposit), THEN they will send you the relevant paperwork. I've talked to three renting scammers through Craig's List... Maybe I can get excellent financing for a new home in the Bay Area? If not, then time to move (again).","tags":"random","url":"not-gonna-move.html","loc":"not-gonna-move.html"},{"title":"Gnuplot Scripting","text":"Information on scripting in gnuplot can be found here , with good examples on how to use these scripts.","tags":"computronics","url":"gnuplot-scripting.html","loc":"gnuplot-scripting.html"},{"title":"Beware of \"-fdefault-real-8\" in gfortran 4.6 on Mac OS X!","text":"Horrible error I found when linking my LAPACK/BLAS code, compiled with gfortran 4.6, with the vecLib framework on Mac OS X 10.6 ( vecLib is the Mac OS X Xcode implementation of BLAS and LAPACK). Garbage results! Lesson to learn: don't use the compiler flag -fdefault-real-8 . From the MAN page on gfortran, -fdefault-real-8 Set the default real type to an 8 byte wide type. Do nothing if this is already the default. This option also affects the kind of non-double real constants like 1.0, and does promote the default width of \"DOUBLE PRECISION\" to 16 bytes if possible, unless -fdefault-double-8 is given, too. Can't figure out why it doesn't work, don't care, won't use it.","tags":"computronics","url":"beware-of-fdefault-real-8-in-gfortran-4-6-on-mac-os-x.html","loc":"beware-of-fdefault-real-8-in-gfortran-4-6-on-mac-os-x.html"},{"title":"Moving Disks Within and Across an LVM2 System","text":"Here's some useful instructions on moving disks around within an LVM2 system. For those new to LVM2, I quote the wikipedia summary : LVM is a logical volume manager for the Linux kernel ; it manages disk drives and similar mass-storage devices, in particular large ones. The term \"volume\" refers to a disk drive or partition thereof. It was originally written in 1998 by Heinz Mauelshagen, who based its design on that of the LVM in HP-UX . So to continue, here is how to do the following: Move Disks Within the System Make sure that you have an up-to-date backup for both the data within the volume group and the volume group configuration. Deactivate the volume group by entering the following command: vgchange -a -n /dev/vol_group_name . Remove the volume group entry from /etc/lvmtab and the associated device files from the system by entering: vgexport /dev/vol_group_name . Next, physically move your disks to their desired new locations. To view their new locations in the logical volume, enter: vgscan -v . Now re-add the volume group entry back to /etc/lvmtab and the associated device files back to the system: Create a new directory for the volume groups with mkdir . Create a group file in the above directory with mknod . Issue the vgimport command: vgimport /dev/vol_group_name physical_volume1_path . Activate the newly imported volume group: vgchange -a y /dev/vol_group_name . Back up the volume group information: vgcfgbackup /dev/vol_group_name . Move Disks Across Systems The procedure for moving the disks in a volume group to different hardware locations on a different system is illustrated in the following example. Suppose you want to move the three disks in the volume group to another system. Follow these steps: Make the volume group and its associated logical volumes unavailable to users. (If any of the logical volumes contain a file system, the file system must be unmounted. If any of the logical volumes are used as secondary swap, you will need to disable swap and reboot the system; for information on secondary swap, see Primary and Secondary Swap .): vgchange -a -n /dev/vg_planning Use vgexport (1M) to remove the volume group information from the /etc/lvmtab file. You can first preview the actions of vgexport with the -p option: vgexport -p -v -m plan_map . With the -m option, you can specify the name of the map file that will hold the information that is removed from the /etc/lvmtab file. This file is important because it will contain the names of all the logical volumes in the volume group. You will use this map file when you set up the volume group on the new system. If the preview is satisfactory, run the command without -p : vgexport -v -m plan_map vg_planning . Now vgexport actually removes the volume group from the system. It then creates the plan_map file. Once the /etc/lvmtab file no longer has the vg_planning volume group configured, you can shut down the system, disconnect the disks, and connect the disks onto the new system. Transfer the file plan_map to the / directory on the new system. On the new system, create a new volume group directory and group file: cd /mkdir /dev/vg_planningcd /dev/vg_planning . When you create the group file, specify a minor number that reflects the group number. Volume group numbering starts at 00 ; the volume group number for the fifth volume group, for example, is 04 : mknod /dev/vg_planning/group c 64 0x040000 Add the disks to the new system. Once you have the disks installed on the new system, type the following command to get device file information for them: ioscan -fun -C disk Now, issue the vgimport command. To preview, use the -p option. To actually import the volume group, re-issue the command omitting the -p : vgimport -p -v -m plan_map /dev/vg_planning /dev/dsk/c6t0d0 /dev/dsk/c6t1d0 /dev/dsk/c6t2d0 Finally, activate the newly imported volume group: vgchange -a y /dev/vg_planning","tags":"computronics","url":"moving-disks-within-and-across-an-lvm2-system.html","loc":"moving-disks-within-and-across-an-lvm2-system.html"},{"title":"AucTex custom directory how-to","text":"Say your local emacs distribution does not include AucTeX , the very nice emacs package for LaTeX. Say you don't have administrative access to your machine. Then how do you install AucTeX (hint, it's not completely obvious)? You install into an user directory of course. Those steps, however, are not completely obvious. Here are my prerequisites: My custom emacs lisp directory are located in ~/.xemacs/elisp . My user texmf tree is located in ~/Library/texmf (I have a Mac OS X version of TexLive , a LaTeX distribution). Here is what to do. Make the necessary changes if you have a different emacs lisp user directory, or different local texmf tree. Download auctex-11.86.tar.gz into some directory. Extract with the command, tar xvfz auctex-11.86.tar.gz && rm auctex-11.86.tar.gz . In the extracted directory (auctex-11.86/), run the following command, ./configure --lispdir=$HOME/.xemacs/elisp --with-texmf-dir=$HOME/Library/texmf Run make && make install , and you're good to go.","tags":"computronics","url":"auctex-custom-directory-how-to.html","loc":"auctex-custom-directory-how-to.html"},{"title":"This American Life -- download and fix","text":"Here's my little script for downloading This American Life episodes. The engine for the code is found from this website . The script for the downloader is shown below: # !/bin/bash num=$( printf \"%03d\" \"$1\" ) ; track=\"$1″ ; year=\"$2″ ; # download file wget -c http://audio.thisamericanlife.org/jomamashouse/ismymamashouse/\"$num\"mp3 -O PRI.ThisAmericanLife.\"$num\".mp3 ; # add metadata metawrite -a \"Ira Glass\" -A \"This American Life\" -T \"$track\" -y \"$year\" PRI.ThisAmericanLife.\"$num\".mp3 ; Warning This is very very old. nprstuff has superseded this functionality with Python code.","tags":"computronics","url":"this-american-life-download-and-fix.html","loc":"this-american-life-download-and-fix.html"},{"title":"Hernandez tanks?","text":"A common(?) technology for organ regeneration in David Marusek's late 22nd-early 23rd century universe (see, e.g., Getting to Know You , Counting Heads , and Mind Over Ship ) is the concept of Hernandez tanks. Interestingly enough, there is a story about an US soldier, Isaias Hernandez, re-growing his leg muscle after an experimental medical procedure . Coincidence?","tags":"random","url":"hernandez-tanks.html","loc":"hernandez-tanks.html"},{"title":"ssh-copy-id","text":"One of the most useful scriptlets I have found in my day-to-day use is ssh-copy-id , a tool to copy an user's SSH public key from one machine to another. The code for this can be found here . However, in case that blog entry disappears, here's the bash. #!/bin/bash # Shell script to install your public key on a remote machine # Takes the remote machine name as an argument. # Obviously, the remote machine must accept password authentication, # or one of the other keys in your ssh-agent, for this to work. ID_FILE = \" ${ HOME } /.ssh/id_rsa.pub\" if [ \"-i\" = \" $1 \" ] ; then shift # check if we have 2 parameters left, if so the first is the new ID file if [ -n \" $2 \" ] ; then if expr \" $1 \" : \".*\\.pub\" >/dev/null ; then ID_FILE = \" $1 \" else ID_FILE = \" $1 .pub\" fi shift # and this should leave $1 as the target name fi else if [ x $SSH_AUTH_SOCK ! = x ] && ssh-add -L >/dev/null 2 > & 1 ; then GET_ID = \" $GET_ID ssh-add -L\" fi fi And that's it!","tags":"computronics","url":"ssh-copy-id.html","loc":"ssh-copy-id.html"},{"title":"Sockso Install on Ubuntu","text":"Very good directions on how to install sockso on Ubuntu shown here . Newest Linux version of sockso can be found there .","tags":"computronics","url":"sockso-install-on-ubuntu.html","loc":"sockso-install-on-ubuntu.html"},{"title":"how to create histogram from GNUPlot","text":"Not quite standard way, but using gnuplot, one can create a histogram from data organized this way, where you have an ascii file with one entry per line. Very good description is found here . I quote the following, kmsmith137 wrote: > How can I make a histogram in gnuplot? From the point of view of the purity, you shouldn't be trying to. The Unix philosophy is \"one tool one task\". gnuplot's task is plotting, not data processing. > I could write a standalone program which preprocesses my sample file, and outputs the bin ranges and counts, but surely there is some way to do the binning internally to gnuplot? There's one, but it's a bit quirky: bw = 5 # substitute what you want bin(x,width)=width*floor(x/width) plot 'data' using (bin($1,bw)):(1.0) smooth freq with boxes See \"help smooth\" to understand what this does. And then you're good to go!","tags":"computronics","url":"how-to-create-histogram-from-gnuplot.html","loc":"how-to-create-histogram-from-gnuplot.html"},{"title":"DLNA Servers for Linux?","text":"Good description for DLNA servers for Linux can be found here . Of course, since I use Ubuntu, this blog article, How to Install DLNA Server on Ubuntu 10.4 , may be of interest as well.","tags":"computronics","url":"dlna-servers-for-linux.html","loc":"dlna-servers-for-linux.html"},{"title":"Just Use Forked-Daapd","text":"Just use forked-daapd for delivering your music remotely, or through your local LAN. I am running Ubuntu Maverick (10.10), and here are instructions that worked for me . Nice DAAP client functionality is enabled in banshee , for both local and remote clients. Local DAAP server are detected automatically. Remote DAAP servers can be found by choosing Media → Add Remote DAAP Server .","tags":"computronics","url":"just-use-forked-daapd.html","loc":"just-use-forked-daapd.html"},{"title":"My Simple Little Fresh Air Downloader (With NPR API)","text":"Here's my newest attempt in downloading NPR programs, specifically NPR's Fresh Air , scripted using bash and joining the following tools: curl and wget to download asx and mp3 files. NPR API , using the NPR API query generator to create custom URLs for NPR's Fresh Air. Good instructions for using the query generator can be found here . sox to join the downloaded mp3 files into a single wav file. Sox mp3 plugin must be included here. ffmpeg to convert the wav file into an AAC file (m4a extension). ffmpeg must be compiled with AAC encoding support, through the faac library . my own executable, metawrite, using the taglib API with mp4 tagging support . to add the following tags: title. author. artist. year. track number. album (year of program). Here is the script: #!/bin/bash # do we have a valid date decdate = $( date --date \" $1 \" +%d.%m.%Y ) ; dayOfMonth = $( date --date \" $1 \" +%A ) ; if [ \" $dayOfMonth \" == \"Saturday\" ] ; then exit 1 ; fi if [ \" $dayOfMonth \" == \"Sunday\" ] ; then exit 1 ; fi if [ ! -n $( echo \" $decdate \" | grep \"invalid date\" ) ] ; then exit 1 ; fi #get the URL wgdate = $( date --date \" $1 \" +%d-%b-%Y ) ; nprApiDate = $( date --date = \" $1 \" +%Y-%m-%d ) ; nprURL = \"http://api.npr.org/query?id=13&date=\" $nprApiDate \"&dateType=story&output=NPRML&apiKey=MDA2OTgzNTcwMDEyOTc1NDg4NTNmMWI5Mg001\" ; # now make sure that the path has included within it location for metawrite # now parse out the data on the URL -- its year, its artist, its # comment, its title (track number not included). Then delete the # source file year = $( date --date \" $1 \" +%Y ) ; titleDate = $( date --date \" $1 \" \"+%A, %B %d, %Y\" ) ; artist = \"Terry Gross\" ; comment = \"more info at : Fresh Air from WHYY and NPR Web site\" ; title = \" $titleDate \" ; touch freshair $wgdate .txt ; rm -f freshair $wgdate .txt ; curl \" $nprURL \" | grep \\< title \\> | grep CDATA | grep -v \"Fresh Air from WHYY\" | sed 's/.*CDATA\\[//g' | sed 's/\\]\\]>.*//g' > freshair $wgdate .txt ; for num in $( seq 1 $( cat freshair $wgdate .txt | wc -l )) ; do subtitle = $( cat freshair $wgdate .txt | head -n $num | tail -n 1 ) ; title = $( echo \" $title \"\"; \" $num \") \"\" $subtitle \" ) ; done album = \"Fresh Air from WHYY and NPR: \" $year ; title = $( echo $title \".\" ) ; rm -f freshair $wgdate .txt ; # now download the URL files using NPR API, using mp3... if [ $year -ge 2003 ] ; then nprApiDate = $( date --date = \" $1 \" +%Y-%m-%d ) ; nprMp3Date = $( date --date = \" $1 \" +%Y%m%d ) ; touch freshair $wgdate .txt ; rm -f freshair $wgdate .txt ; for file in $( curl \" $nprURL \" | grep m3u | sed 's/.*http/http/g' | \\ grep http | sed 's/\\.m3u\\?.*/\\.m3u/g' ) ; do curl \" $file \" | grep http | sed 's/\\.mp3<.*/\\.mp3/g' >> freshair \" $wgdate \" .txt ; done num = 1 for file in $( cat freshair $wgdate .txt | grep $nprMp3Date | sort | uniq ) ; do wget -c \" $file \" -O freshair. \" $decdate \" . \" $num \" .mp3 ; num = $(( $num + 1 )) ; done rm -f freshair $wgdate .txt ; # now concatenate mp3 files into one wav file, then delete mp3 files... ( for file in freshair. \" $decdate \" .*.mp3 ; do sox $file -t cdr - ; done ) | \\ sox -t cdr - freshair $wgdate .wav && rm -f freshair. \" $decdate \" .*.mp3 ; else # now download the URL files into wav files num = 1 for mmsurl in $( cat freshair. \" $decdate \" .asx | grep \"mms://\" | \\ sed 's/.*mms/mms/g' | sed 's/\\.wma.*/\\.wma/g' ) ; do echo \" $mmsurl \" ; mplayer -ao pcm:file = freshair. \" $decdate \" . \" $num \" .wav \\ -channels 2 -af resample = 44100 :0:0 $mmsurl ; num = $(( $num + 1 )) ; done # concatenate the wav file into one wav file, then convert into m4a, # then add tagging information ( for file in freshair. \" $decdate \" .*.wav ; do sox $file -t cdr - ; done ) | \\ sox -t cdr - freshair $wgdate .wav && rm -f freshair. \" $decdate \" .*.wav ; fi # now convert the files into m4a /usr/bin/avconv -y -i freshair $wgdate .wav -ar 44100 -ac 2 -aq 400 \\ -acodec libfaac NPR.FreshAir. \" $decdate \" .m4a ; /usr/local/bin/metawrite -t \" $title \" -a \" $artist \" -A \" $album \" -y \" $year \" -c \" $comment \" \\ NPR.FreshAir. \" $decdate \" .m4a ; /usr/bin/mp4tags -P /mnt/media/freshair/npr_freshair_image_300.png \\ NPR.FreshAir. \" $decdate \" .m4a ; rm -f freshair $wgdate .wav freshair. \" $decdate \" .asx ; if [ $# -ge 2 ] ; then /usr/bin/mp4tags -t \" $2 \" NPR.FreshAir. \" $decdate \" .m4a ; fi if [ $# -ge 3 ] ; then /usr/bin/mp4tags -t \" $2 \" -T \" $3 \" NPR.FreshAir. \" $decdate \" .m4a ; fi","tags":"computronics","url":"my-simple-little-fresh-air-downloader-with-npr-api.html","loc":"my-simple-little-fresh-air-downloader-with-npr-api.html"},{"title":"I hate US immigration","text":"No surprises heres, I hate US immigration. Dealing with them in any way makes me wish, \"why didn't I move to Singapore?\" or, \"thank God for this recession, at least things are getting cheaper.\" First, my wife's interview for adjustment of status has come through -- only 4 months after I filed all her applications. Nice of them to give us 1 month to collect all the documents, but whatever... Nice of them to require an updated medical examination, which means I had to shell out $190 in expenses to go through all the vaccinations and medical rigmarole I went through before. It's great that people have to pay out of pocket for immigration related medical examinations, and that seemingly only the shittiest clinics are authorized to perform mandatory immigration-related medical exams. Why do I even pay for insurance, or pay my taxes?? But there's more... Now I have to get my wife an HPV vaccination, and send the documentation for performing this vaccination to the clinic that performed the medical examination. So I have to arrange an appointment with the gynecologist ($15), get the HPV vaccination which is medically required ONLY for women under 26 ($15), and drive back home AND then back to the clinic to deliver a GODDAMN SHEET OF FUCKING PAPER ($5) . One might think that I had not done due diligence, perhaps the US immigration website has this information readily available. Or perhaps it has a customer service line that isn't a Kafka-esque exercise in government waste and stupidity. Or perhaps there are US authorized immigration lawyers and doctors that provide a consistent set of information on what is required with these forms, with medical exams. NO!","tags":"random","url":"i-hate-us-immigration.html","loc":"i-hate-us-immigration.html"},{"title":"More GStreamer stuff","text":"This is more stuff on GStreamer usage. Here's the code to extract tag information from the GStreamer webcvs website: /* GStreamer * Copyright (C) 2003 Thomas Vander Stichele * 2003 Benjamin Otte * 2005 Andy Wingo * 2005 Jan Schmidt * * gst-metadata.c: Use GStreamer to display metadata within files. * * This library is free software; you can redistribute it and/or * modify it under the terms of the GNU Library General Public * License as published by the Free Software Foundation; either * version 2 of the License, or (at your option) any later version. * * This library is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU * Library General Public License for more details. * * You should have received a copy of the GNU Library General Public * License along with this library; if not, write to the * Free Software Foundation, Inc., 59 Temple Place - Suite 330, * Boston, MA 02111-1307, USA. */ #ifdef HAVE_CONFIG_H # include \"config.h\" #endif #include #include #include #include char * filename = NULL ; GstElement * pipeline = NULL ; GstElement * source = NULL ; #define NEW_PIPE_PER_FILE static gboolean message_loop ( GstElement * element , GstTagList ** tags ) { GstBus * bus ; gboolean done = FALSE ; bus = gst_element_get_bus ( element ); g_return_val_if_fail ( bus != NULL , FALSE ); g_return_val_if_fail ( tags != NULL , FALSE ); while ( ! done ) { GstMessage * message ; message = gst_bus_pop ( bus ); if ( message == NULL ) /* All messages read, we're done */ break ; switch ( GST_MESSAGE_TYPE ( message )) { case GST_MESSAGE_ERROR : case GST_MESSAGE_EOS : gst_message_unref ( message ); return TRUE ; case GST_MESSAGE_TAG : { GstTagList * new_tags ; gst_message_parse_tag ( message , & new_tags ); if ( * tags ) * tags = gst_tag_list_merge ( * tags , new_tags , GST_TAG_MERGE_KEEP ); else * tags = new_tags ; break ; } default : break ; } gst_message_unref ( message ); } gst_object_unref ( bus ); return TRUE ; } static void make_pipeline () { GstElement * decodebin ; if ( pipeline != NULL ) gst_object_unref ( pipeline ); pipeline = gst_pipeline_new ( NULL ); source = gst_element_factory_make ( \"filesrc\" , \"source\" ); g_assert ( GST_IS_ELEMENT ( source )); decodebin = gst_element_factory_make ( \"decodebin\" , \"decodebin\" ); g_assert ( GST_IS_ELEMENT ( decodebin )); gst_bin_add_many ( GST_BIN ( pipeline ), source , decodebin , NULL ); gst_element_link ( source , decodebin ); } static void print_tag ( const GstTagList * list , const gchar * tag , gpointer unused ) { gint i , count ; count = gst_tag_list_get_tag_size ( list , tag ); for ( i = 0 ; i < count ; i ++ ) { gchar * str ; if ( gst_tag_get_type ( tag ) == G_TYPE_STRING ) { if ( ! gst_tag_list_get_string_index ( list , tag , i , & str )) g_assert_not_reached (); } else { str = g_strdup_value_contents ( gst_tag_list_get_value_index ( list , tag , i )); } if ( i == 0 ) { g_print ( \" %15s: %s \\n \" , gst_tag_get_nick ( tag ), str ); } else { g_print ( \" : %s \\n \" , str ); } g_free ( str ); } } int main ( int argc , char * argv []) { guint i = 1 ; setlocale ( LC_ALL , \"\" ); gst_init ( & argc , & argv ); if ( argc < 2 ) { g_print ( \"Please give filenames to read metadata from \\n\\n \" ); return 1 ; } make_pipeline (); while ( i < argc ) { GstStateChangeReturn sret ; GstState state ; GstTagList * tags = NULL ; filename = argv [ i ]; g_object_set ( source , \"location\" , filename , NULL ); GST_DEBUG ( \"Starting reading for %s\" , filename ); /* Decodebin will only commit to PAUSED if it actually finds a type; * otherwise the state change fails */ sret = gst_element_set_state ( GST_ELEMENT ( pipeline ), GST_STATE_PAUSED ); if ( GST_STATE_CHANGE_ASYNC == sret ) { if ( GST_STATE_CHANGE_SUCCESS != gst_element_get_state ( GST_ELEMENT ( pipeline ), & state , NULL , 5 * GST_SECOND )) { g_print ( \"State change failed for %s. Aborting \\n \" , filename ); break ; } } else if ( sret != GST_STATE_CHANGE_SUCCESS ) { g_print ( \"%s - Could not read file \\n \" , filename ); goto next_file ; } if ( ! message_loop ( GST_ELEMENT ( pipeline ), & tags )) { g_print ( \"Failed in message reading for %s \\n \" , argv [ i ]); } if ( tags ) { g_print ( \"Metadata for %s: \\n \" , argv [ i ]); gst_tag_list_foreach ( tags , print_tag , NULL ); gst_tag_list_free ( tags ); tags = NULL ; } else g_print ( \"No metadata found for %s \\n \" , argv [ i ]); sret = gst_element_set_state ( GST_ELEMENT ( pipeline ), GST_STATE_NULL ); #ifndef NEW_PIPE_PER_FILE if ( GST_STATE_CHANGE_ASYNC == sret ) { if ( GST_STATE_CHANGE_FAILURE == gst_element_get_state ( GST_ELEMENT ( pipeline ), & state , NULL , GST_CLOCK_TIME_NONE )) { g_print ( \"State change failed. Aborting\" ); break ; } } #endif next_file : i ++ ; #ifdef NEW_PIPE_PER_FILE make_pipeline (); #endif } if ( pipeline ) gst_object_unref ( pipeline ); return 0 ; }","tags":"computronics","url":"more-gstreamer-stuff.html","loc":"more-gstreamer-stuff.html"},{"title":"converting video media files into iPod format using GStreamer","text":"Here, I convert video media files into ipod-friendly mp4 format using gstreamer 0.10. One requires the gstreamer faac plugin to encode into AAC format as well as the gstreamer ffmpeg plugin to mux and demux from a variety of different audio formats. gst-launch filesrc location=\"$input\" ! decodebin name=decoder decoder. ! \\ videoscale ! videorate ! ‘video/x-raw-yuv,width=320,height=240,framerate=(fraction)2997/100' ! \\ ffmpegcolorspace ! ffenc_mpeg4 bitrate=640000 ! queue ! ffmux_mp4 name=mux ! \\ filesink location=$output\".mp4 decoder. ! audioconvert ! faac bitrate=32000 ! mux. This converts the input video file into an mp4 movie file with 640 kb/s video at a resolution of 320x240 and 29.97 frames per second, and an audio portion at 32 kb/s audio. The video is encoded with divx-mpeg4 and the audio with aac.","tags":"computronics","url":"converting-video-media-files-into-ipod-format-using-gstreamer.html","loc":"converting-video-media-files-into-ipod-format-using-gstreamer.html"},{"title":"Converting DVD title into an AVI file, using GStreamer 0.10","text":"Figured out how to convert a DVD title into an AVI file, using GStreamer 0.10. Here $N is the chapter number, and $outfile is the name of the output file. gst-launch dvdreadsrc title=$N ! decodebin name=decoder decoder. ! videoscale ! videorate ! \\ 'video/x-raw-yuv,framerate=(fraction)2997/100' ! ffmpegcolorspace ! \\ ffenc_mpeg4 bitrate=2400000 ! queue ! avimux name=muxer ! \\ filesink location=\"$outfile\".avi decoder. ! audioconvert ! \\ audioresample ! 'audio/x-raw-int,rate=44100,channels=2' ! lame bitrate=128000 ! \\ queue ! muxer. Here, I found a way to set the audio output to 44100 Hz and stereo (2 channels). I convert the DVD to an AVI file with a 2400k video bitrate and 128k audio bitrate, at a 29.97 (NTSC) framerate. The video is encoded into divx mpeg4, and the audio into mp3.","tags":"computronics","url":"converting-dvd-title-into-an-avi-file-using-gstreamer-0-10.html","loc":"converting-dvd-title-into-an-avi-file-using-gstreamer-0-10.html"},{"title":"LVM \"stuff\"","text":"Crappy crappy day. I just spent nine hellish hours trying to fix my LVM partition with hard drive. Here are some things I have learned. Do not put a root partition in LVM . If LVM gets hosed, there is a very good chance your system can become unrecoverable. Put the extra hard drives into IDE→USB enclosures. This way, I don't have to worry about the \"joy\" of how the hell the BIOS figures out what drives are where. If I feel like it, later tonight I'll just create a very minimal drive configuration, then go to bed. And to think, today I planned on job-hunting!","tags":"computronics","url":"lvm-stuff.html","loc":"lvm-stuff.html"},{"title":"I am free!!","text":"I have finally completed my horrible, horrible thesis. The format is (la rgely) frozen. However, chapters 1-5 seem to be fine. Chapters 6 and 7 (the final chapters), with associated appendices D and E, will be proofread and the resulting corrections sent within 10 days. Still, a lot of work needs to be done (outside the thesis)... setting up my thesis committee. Who will sit on it, how will it be arranged, etc. Since some admin people want me to get a joint degree from the University of Virginia and the École Normale Supérieure (however that might be arranged), I might have the committees of BOTH schools plus some \"glue\" members to satisfy both requirements. Ha! Reading is fun! Although to be fair, not as \"fun\" as writing the thesis, checking all the math, etc. I'll have to get a job somewhere... But at least the horror is over. Here's a palette of the final 6 plots of my thesis. It took six days for the data to compile on several machines. I also did a scan of my thesis files (including bibliography). It comes down to: 503,696 characters, 55,436 words, and 5804 lines.","tags":"random","url":"i-am-free.html","loc":"i-am-free.html"},{"title":"Barbeque","text":"I just had a barbecue at my place, 2 September - 3 September 2007. My friends will have online pics of it. I cooked a metric load of taco salad, garlic cheese pasta with onion and tomato sautee, garlic and olive oil guacamole (avocadoes), about half a chicken barbecued, and 8 steaks. We drank a few gallons of sangria that friends of mine made, watched The IT Crowd , watched the moon rise, blew up some fireworks, watched some random TV, watched Burn Notice , and finally went to bed. The next morning we made Turkish sludge coffee, ate my birthday cake for breakfast, and some of us picked peppers, bittermelon, and wild mint from our garden and lawn. All in all, it was a good barbecue. I wish I could do this more often. Today I am going to finish my thesis, and then tomorrow I am going to submit.","tags":"random","url":"barbeque.html","loc":"barbeque.html"},{"title":"Lucky...","text":"I am very very lucky, and I feel very very lazy. My parents are downstairs proofreading my thesis, while I am upstairs blogging. And watching Law and Order on their projector TV.","tags":"random","url":"lucky.html","loc":"lucky.html"},{"title":"Tribute to Inessentiality","text":"Disclaimer: I am addicted to this cereal). You can find out more about this cereal on Wikipedia . All pictures are copyrighted by, I assume, the Kellogg's Corporation. Disturbingly enough, one can purchase this cereal on Amazon.com .","tags":"random","url":"tribute-to-inessentiality.html","loc":"tribute-to-inessentiality.html"},{"title":"Gotten beagle to search through my KMail folders","text":"I have finally gotten beagle to search through my KMail folders, thanks to the very useful and kind help of the programmer of the KMail backend to beagle . I am using Ubuntu Edgy (6.10), which ships with beagle-0.2.9 -- a very very buggy release of beagle. He suggested I use the latest version, beagle-0.2.16 , and now everything works! Instructions to install and use the latest version of beagle are very simple: shut down beagled with beagle-shutdown . installation instructions for Ubuntu Edgy can be found here . You need to add the following line to your sources.list file, deb http://beagle-project.org/files/ubuntu/edgy/ ./\" update and upgrade your system. I use sudo apt-get update && sudo apt-get upgrade . You should now have the latest version of beagle with appropriate dependencies. restart beagle in the background with the command, beagled --bg . Now searching with beagle will ALSO search through your KMail directories. Happy hunting!","tags":"computronics","url":"gotten-beagle-to-search-through-my-kmail-folders.html","loc":"gotten-beagle-to-search-through-my-kmail-folders.html"},{"title":"New drive has arrived!","text":"Well, I finally received my ~650 gigabyte Seagate external drive, and added it to my LVM2 partition. So now I have nearly 1 TERABYTE (981.14 gigabytes, to be exact) of space on my hard drive. The picture of the drive is shown on the bottom left. Second, I have a media partition with 874 gigs, and a home partition with 30 gigs, a software partition with 55 gigs. This is shown in the figure on the right.","tags":"computronics","url":"new-drive-has-arrived.html","loc":"new-drive-has-arrived.html"},{"title":"new hard drive","text":"Well, I just purchased a new external hard drive. 650 gigs. This should fit nicely with the 370 gigs I currently have, transparently arranged via LVM. This should fit in nicely with the 375.66 gigs that I currently have, distributed among 4 disk drives. This was more of an impulse buy -- I was running out of space to hold my collection of software (18/25 gigs) and media files (301/319 gigs). I hope I finally get my money from France as well. It has taken entirely too long to just...simply...transfer my money from there in one big go. I tried the international wire transfer from their end -- it took 2 weeks of constantly calling them, and having a friend see what was going on, before I saw that the wire was cancelled. And of course, with no feedback from the shittiest bank in Western Europe, the Caisse-Epargne . So I decided to cash a French check, from the saintedly efficient Caisse, in my local credit union. Say what you will about the US, things actually get done here. You couldn't buy service as efficient and helpful as the DMV in a domestic French company. Of course, one doesn't appreciate it until one goes out and sees for himself.","tags":"computronics","url":"new-hard-drive.html","loc":"new-hard-drive.html"},{"title":"Liberal Education","text":"This is an interesting article, from Harper's magazine. It talks about the perils of a liberal education. Hmm. Wonder what that might be.","tags":"random","url":"liberal-education.html","loc":"liberal-education.html"},{"title":"apartment picture Paris","text":"This is a picture of my apartment in Paris. It looks OK. OK, it looks entirely too small. I wish my apartment was bigger, but right now I think I have a good deal, considering I live in the middle of the fifth arrondisement (Latin Quarter) -- so yeah.","tags":"random","url":"apartment-picture-paris.html","loc":"apartment-picture-paris.html"},{"title":"Amarok gets more awesome","text":"Oh my goodness. I had no idea what last.fm feature in Amarok was or why i should care. Then I tried the last.fm \"neighborhood radio,\" and it's pretty god-damn amazing. Seamless and flawless with Amarok -- it's essentially the Alternative radio station I was looking for on the web, but...better 😊","tags":"computronics","url":"amarok-gets-more-awesome.html","loc":"amarok-gets-more-awesome.html"},{"title":"logjam broken! Linux VOIP fixed","text":"registrar: proxy01.sipphone.com User: USER ACCOUNT Password: USER PASSWORD Authentication login: USER ACCOUNT Realm/Domain: proxy01.sipphone.com And then, say I want to dial a landline number, say 1-804-555-1234, I would then place a call on ekiga as 18045551234@proxy01.sipphone.com , and then I would be good to go!","tags":"computronics","url":"logjam-broken-linux-voip-fixed.html","loc":"logjam-broken-linux-voip-fixed.html"},{"title":"I am growing a beard","text":"I figured that shaving is too much work, so I'm growing a beard. One whim that I have is to dye my hair salt-and-pepper gray, or perhaps completely gray. Maybe do the same to my beard as well, if I decide to keep it.","tags":"random","url":"i-am-growing-a-beard.html","loc":"i-am-growing-a-beard.html"},{"title":"aaghh","text":"I wish I weren't so poor! Just got this deal in from tigerdirect.com, a DECENT (actually, VERY decent) Acer laptop for only $400!! Got the offer today, and the deal ends tomorrow. But alas, I am poor. Otherwise, I would purchase it with my debit card and have it delivered to my parents' house -- a nice gift for when I get back home. Btw, here's the soon to be obsolete link here .","tags":"random","url":"aaghh.html","loc":"aaghh.html"},{"title":"Long-Winded Update","text":"Well, I got the ALSA sound capture to work properly on my ubuntu machine @ work. Now the trick is to get the openwengo client to work properly on my linux machine. It doesn't seem to recognize my sound output for some reason. Who knows why? In other news: 2nd paper finished, 3rd paper almost finished, 4th paper to be done \"soon,\" and finishing writing up my thesis. Also, have to prepare talks and some other conference proceedings. Big one going to be the IAS meeting in Prague; then APS Division of Plasma Physics meeting sometime in October. Maybe I'll live @ home for the next few months. It sounds attractive 😊","tags":"random","url":"long-winded-update.html","loc":"long-winded-update.html"},{"title":"KDE 3.5 is PERFECT","text":"Everything is there. I really liked KDE, except the panel (in previous versions) would put all the minimized windows in one toolbar. Now, I can group minimized windows by workspace. KDE Works Perfectly.","tags":"computronics","url":"kde-3-5-is-perfect.html","loc":"kde-3-5-is-perfect.html"},{"title":"but i came to Paris to ESCAPE bureaucracy!","text":"Oh my god, the administrative bullshit I have to endure has even the native Parisiennes and Parisians complaining! I have to go tomorrow to the Office of Work (Bureau de Travail) on the other side of the city, which is conveniently open 2 hours each weekday to get my work permit, in the perfectly convenient hours of 9 AM – 11 AM. For that I need 4 pieces of documentation: copies of my passport. copies of my stay permit (titre de sejour). copies of my letter of sponsorship from the ENS. copies of my work card from the Paris Observatory. All to get my carte de travail. Then I have to get a French social security card. This will involve the following 6 pieces of information: copy of my carte travail and copy of my titre de sejour. a copy of my most recent and next most recent salary statement (how convenient, as I haven't gotten paid yet b/c...i don't have this piece of documentation). a RIB (Releve d'identite bancaire, like a bank statement, but much more informative) An affidavit of birth, stating where and when I was born. a \"Justificatif de domicile,\" a letter from my landlord/landlady that I live in my apartment, since my signed lease is not actually sufficient proof that I live in the place I pay rent for. And then, perhaps I will get paid!","tags":"random","url":"but-i-came-to-paris-to-escape-bureaucracy.html","loc":"but-i-came-to-paris-to-escape-bureaucracy.html"},{"title":"It's been a while, I think, and here is the work so far","text":"New papers characterizing the compressible limit of the MVI will be out soon, as well as instabilities with large electron thermal conductivities as well as large ion resistivities. Perhaps I can include the effects of finite compressibility as well with them. In a perfect world, I would be able to move onto numerical simulations something like...six months ago. But the world is not perfect, and I am very stupid.","tags":"random","url":"its-been-a-while-i-think-and-here-is-the-work-so-far.html","loc":"its-been-a-while-i-think-and-here-is-the-work-so-far.html"},{"title":"books by Alastair Reynolds","text":"I have ordered books by Alastair Reynolds from amazon.com: 1) Absolution Gap (book 4 in the Revelation Space series), and 2) Diamond Dogs/Turquoise Days (two novellas bound in a book). This is actually a bad idea, since I have pretty much read all the (good) parts of Absolution Gap during my treks to the local B&N and Turquoise (what a hard to type word!) Days is already in a collection of Dozois's Year's Best Science Fiction (I forget which year or edition, suffice to say I have it). The only new book is the Diamond Dogs — it should be very very interesting. Reynolds writes for those with a deep science/mathematics background. Weird weapons from the extant remains of alien cultures -- hypometric devices (the inner parts look like Escher's famous ladder painting), bladder mines, inertial suppression devices, and then he delves into really exotic weapons (neutron star \"computer,\" the gravity wave antenna built by the Scuttlers). Pattern Jugglers, Shrouders, Clown Jumpers, grubs, Scuttlers, Nestbuilders, and the ever-present Inhibitors -- a very powerful automated system of machines that suppresses the emergence of spacefaring cultures, such that any surviving race does so by becoming very...very...very...very quiet.","tags":"random","url":"books-by-alastair-reynolds.html","loc":"books-by-alastair-reynolds.html"},{"title":"Start of a story","text":"This is a scrap of an idea I thought up over sixteen years ago. This war has been going on for so long it doesn't even matter except to bean counters, employees of the GAO (General Accounting Office) and the Army who have to explain the short and long term financial costs to an increasingly resigned body politic. This conflict has dragged on for so long, and become so muddy, that a whole mythology has sprung up around it. These Desert Warriors have fought for as long as they can remember against the Infidel. Each victory is like a blessing from God, delivered by Gabriel, to continue the fight. Each defeat, each martyr, leads to ten or more rising up to take their place. They cannot lose. Our proxies have left the fight decades ago. Now it's just our drones against them, cleaner but bloodier. On our side, the war had soon left the realm of simple platitudes about freedom and democracy for our soldiers, or the drier arena of treaties and \"police actions\" for our diplomats and politicians. The president who got us into this mess was assassinated; some thought that we finally had our own martyr, the Good Martyr, others that the war had gotten beyond pointless. There was open conflict, rioting even, in American cities. The coldest comfort is that, for whatever reason, we are more sophisticated in the mythology we have constructed. Rather than the one point of view that unites the Arabs, we have two or more: to subdue the infidels, to leave them alone, even to kill them all. Skeptics used to say the war used to be about oil, except all the cheap extractable oil was gone years ago, first by a well-timed backpack nuke around Basra, and then whatever was left by a carefully engineered bacterial culture that eats the oil at a promiscuous rate. We used to not know whether many of these oil deposits were contiguous, but now we do; Saudi Arabia has ten more years left before it's all gone, Kuwait maybe five. All the Gulf States have at most twenty years left before the germ eats it all up. The bacterium, since it's so metabolically active, produces megatons of gas per week, the result being that a toxic cloud of useless petroleum byproducts hangs over most of the oil fields within the region.","tags":"book","url":"start-of-a-story.html","loc":"start-of-a-story.html"},{"title":"Birthday","text":"A person named Vinnie asks if we can switch apartments. I told him I'll take a look this coming Monday and decide whether I want him to or not. He thinks it's all a formality, what with my checking of the apartments and what-not, but it may be too much trouble with my already having set up the driver's license and mail forwarding and voter registration and such to make the change from one apartment to another. Oh well, we'll see...","tags":"random","url":"birthday.html","loc":"birthday.html"},{"title":"nostalgic for my Caltech \"experience\"","text":"I love Firefox and Mozilla-based web browsers. They allow me to look at email from the beginning of time (winter 1996-1997). Here's an oldie, but a goodie, from back when I applied to grad schools in physics. Here's some context: my handwriting is atrocious, so the return letter for my grad applications was thought to be mail code 543 instead of 593 (my 9's look like 4's, get it?). Everyone except for Princeton accepted me, but also everyone else sent my mail to the wrong code. Here's an email from the person who gets mail at 543: Tanim Apparently U. Virginia thinks your MSC is 543, which I'm pretty sure it's not. A letter addressed to you from the physics department there was sent to my mailing code. I'm sure it's important, like some kind of love letter or bomb or something. I had this pen-pal once who wrote me a wonderful letter about the delicate art of prison rape. Or was that just a brochure? Anyway, unless you want me to do something special to the letter you can find it outside my room in Ricketts. Room ROTO (27) in Snatch. - Ben Gudlewski","tags":"random","url":"nostalgic-for-my-caltech-experience.html","loc":"nostalgic-for-my-caltech-experience.html"},{"title":"Happy Thoughts","text":"I have listened to the newest Stereolab CD, Margerine Eclipse , and it is better than anything I have seen put into audio format in my entire life. I recommend this CD to anyone — people who like Stereolab, people who haven't heard of them before, people who are deaf. It is so good I would recommend it to deaf people.","tags":"random","url":"happy-thoughts.html","loc":"happy-thoughts.html"},{"title":"our man in Nanjing","text":"Yes, my best friend Ben is coming back from Nanjing, PRC, at the end of this month (June). I am so happy. Oh frabjous day, week, month, or year. There will be quite a bit of drugging out right when he gets back. And then maybe a road trip to Gaysville, VA.","tags":"random","url":"our-man-in-nanjing.html","loc":"our-man-in-nanjing.html"}]};